{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpVtYVpApnEGsbZvFOcZ32",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/innovationcopilot/Chain-of-ThoughtsPapers/blob/main/0_BHIxTRF_ALL_NOTEBOOKS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. INITIAL SETUP AND ANALYSIS (CEO)"
      ],
      "metadata": {
        "id": "HBXHQSXflfxF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQyqdD3ZkSnW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "!pip install pycountry\n",
        "import pycountry\n",
        "\n",
        "# Generate a list of country names\n",
        "country_names = [country.name for country in pycountry.countries]\n",
        "\n",
        "# Load the CSV data\n",
        "file_path = '/content/master_people_data.csv'  # Adjust the file path as necessary\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "def identify_gender(name):\n",
        "    name_str = str(name)\n",
        "    return 'M' if 'Mr.' in name_str else 'F' if 'Mrs.' in name_str or 'Ms.' in name_str else 'N/A'\n",
        "\n",
        "# Rename the 'PERMID' column to 'permid' for consistency\n",
        "data.rename(columns={'co_id': 'ciq_id'}, inplace=True)\n",
        "\n",
        "# Apply the identify_gender function to the 'Name' column\n",
        "data['Gender'] = data['Name'].apply(identify_gender)\n",
        "\n",
        "# Handle missing age values\n",
        "data['Age'] = data['Age'].replace('--', 'N/A').fillna('N/A')\n",
        "\n",
        "# Assign unique IDs to individuals based on their name and age\n",
        "data['person_id'] = data.groupby(['Name', 'Age']).ngroup()  # Updated this line\n",
        "\n",
        "# Step 1: Define Regular Expression for CEO Identification\n",
        "ceo_regex = re.compile(r'\\b(?:Chief Executive Officer|CEO)\\b', re.IGNORECASE)\n",
        "\n",
        "# Step 2: Apply the Regular Expression to Flag CEOs\n",
        "data['CEO_Flag'] = data['Position'].apply(lambda x: bool(ceo_regex.search(str(x)))).astype(int)\n",
        "\n",
        "# Function to check for 'group' or any country name in the 'Position' column\n",
        "def check_position(position):\n",
        "    if 'group' in position.lower():\n",
        "        return 0\n",
        "    if any(country.lower() in position.lower() for country in country_names):\n",
        "        return 0\n",
        "    return 1\n",
        "\n",
        "# Apply the function to update the 'CEO_Flag' column\n",
        "data['CEO_Flag'] = data['Position'].apply(lambda x: check_position(str(x))) * data['CEO_Flag']\n",
        "\n",
        "# Step 3: Sort Data\n",
        "data_sorted = data.sort_values(by=['CEO_Flag','person_id', 'Start_Year'])\n",
        "\n",
        "# Calculate the previous job title based on unique person_id\n",
        "data_sorted['Prev_Title'] = data_sorted.groupby('person_id')['Position'].shift()\n",
        "\n",
        "# Calculate the officer start year\n",
        "officer_start_year = data_sorted.groupby(['person_id', 'ciq_id'])['Start_Year'].min().reset_index()\n",
        "officer_start_year.columns = ['person_id', 'ciq_id', 'Officer_Start_Year']\n",
        "\n",
        "# Merge the officer start year data with the main data\n",
        "data_merged = pd.merge(data_sorted, officer_start_year, on=['person_id', 'ciq_id'], how='left')\n",
        "\n",
        "def extract_info(data):\n",
        "    # Initialize an empty list to store the extracted information\n",
        "    info_list = []\n",
        "\n",
        "    col_list = ['permid', 'ciq_id', 'company', 'person_id', 'CEO_Flag', 'ceo', 'ceo_title', 'start_year', 'start_mo', 'end_year', 'end_mo', 'prev_job_id', 'prev_title', 'officer_start_yr', 'director_start_yr', 'gender', 'age', 'affiliations']\n",
        "\n",
        "    # Iterate through the rows of the CSV data\n",
        "    for index, row in data.iterrows():\n",
        "        # Create a dictionary to store the information for the current row\n",
        "        info_dict = {col: None for col in col_list}  # Initialize all columns with None\n",
        "\n",
        "        info_dict.update({\n",
        "            'ciq_id': row.get('ciq_id', \"\"),\n",
        "            'person_id': row.get('person_id', \"\"),\n",
        "            'CEO_Flag': row.get('CEO_Flag', \"\"),\n",
        "            'ceo': row.get('Name', \"\"),\n",
        "            'ceo_title': row.get('Position', \"\"),\n",
        "            'start_year': row.get('Start_Year', \"\"),\n",
        "            'end_year': row.get('End_Year', \"\"),\n",
        "            'gender': row.get('Gender', \"\"),\n",
        "            'age': row.get('Age', \"\"),\n",
        "            'prev_title': row.get('Prev_Title', \"\"),\n",
        "            'officer_start_yr': row.get('Officer_Start_Year', \"\"),\n",
        "            'affiliations': \"N/A\"  # Since affiliations info is not available in the CSV data\n",
        "        })\n",
        "\n",
        "        # Append the info_dict to info_list\n",
        "        info_list.append(info_dict)\n",
        "\n",
        "    # Convert info_list to a DataFrame\n",
        "    info_df = pd.DataFrame(info_list, columns=col_list)\n",
        "\n",
        "    return info_df\n",
        "\n",
        "# Get the extracted information DataFrame\n",
        "info_df = extract_info(data_merged)\n",
        "\n",
        "pd.options.display.max_rows = 99\n",
        "pd.options.display.max_columns = 99\n",
        "\n",
        "info_df.head(25)\n",
        "info_df.describe()\n",
        "\n",
        "#DF with all people, with CEO flag\n",
        "#info_df.to_csv('people.csv', index=False)\n",
        "#pickle.dump(info_df, open(\"info_df.p\", \"wb\"))\n",
        "\n",
        "# Keep only CEOs\n",
        "ceo_final = info_df[info_df['CEO_Flag'] != 0]\n",
        "\n",
        "# Sort ceo_final based on person_id and start_year in descending order\n",
        "ceo_final = ceo_final.sort_values(by=['person_id', 'start_year'], ascending=[True, False])\n",
        "\n",
        "# Drop duplicate entries based on person_id, keeping the first occurrence (later start date)\n",
        "ceo_final = ceo_final.drop_duplicates(subset='person_id', keep='first')\n",
        "\n",
        "# Drop these indices from the DataFrame\n",
        "indices_to_drop = ceo_final.loc[ceo_final['end_year'].notnull()].index\n",
        "ceo_current_final = ceo_final.drop(indices_to_drop)\n",
        "#ceo_current_final.to_csv('ceos_current_final.csv', index=False)\n",
        "#pickle.dump(ceo_current_final, open(\"ceos_current_final.p\", \"wb\"))\n",
        "\n",
        "# Reset the index of ceo_final (optional)\n",
        "ceo_final.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Output the final DataFrame to CSV and pickle files\n",
        "ceo_final.to_csv('ceo_final.csv', index=False)\n",
        "pickle.dump(ceo_final, open(\"ceo_final.p\", \"wb\"))\n",
        "\n",
        "###TO DO - create unique identifiers here and then use the CEO map on this"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create a New DataFrame for Tracking Duplicate Entries\n",
        "ceo_dup = ceo_final.copy()\n",
        "\n",
        "# Sort the DataFrame and Reset the Index\n",
        "ceo_dup.sort_values(['company','start_year','start_mo'], ascending=False, inplace=True)\n",
        "ceo_dup.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Initialize a new column to track duplicate entries\n",
        "ceo_dup['dup_bool'] = False  # False indicates not a duplicate\n",
        "\n",
        "# Step 2: Identify Duplicate Entries\n",
        "for index, row in ceo_dup[:-1].iterrows():  # last row will throw out of range\n",
        "    if ceo_dup.loc[index].permid == ceo_dup.loc[index+1].permid and ceo_dup.loc[index].person_id == ceo_dup.loc[index+1].person_id:\n",
        "        ceo_dup.loc[index+1,'end_year'] = ceo_dup.loc[index,'end_year']\n",
        "        ceo_dup.loc[index+1,'end_mo'] = ceo_dup.loc[index,'end_mo']\n",
        "        ceo_dup.loc[index, 'dup_bool'] = True  # Mark this row as a duplicate\n",
        "\n",
        "# Step 3: Filter Out Duplicate Entries\n",
        "ceo_dedup = ceo_dup[ceo_dup['dup_bool'] != True].drop('dup_bool', axis=1)\n",
        "\n",
        "# Output the final DataFrame to CSV and pickle files\n",
        "#ceo_dedup.to_csv('ceo_dedup.csv', index=False)\n",
        "pickle.dump(ceo_dedup, open(\"ceo_dedup.p\", \"wb\"))\n",
        "\n",
        "# Ensure 'end_year' is in numeric format, non-numeric values will be converted to NaN\n",
        "ceo_dedup['end_year'] = pd.to_numeric(ceo_dedup['end_year'], errors='coerce')\n",
        "\n",
        "# Filter out rows where 'end_year' is 2012 or earlier, but keep rows where 'end_year' is NaN\n",
        "ceo_final_v2 = ceo_dedup.loc[(ceo_dedup['end_year'] >= 2008) | (ceo_dedup['end_year'].isna())]\n",
        "\n",
        "# Group by 'ciq_id' and filter out groups with 3 or more occurrences\n",
        "ceo_final_v2 = ceo_final_v2.groupby('ciq_id').filter(lambda x: len(x) < 3)\n",
        "\n",
        "# Now, filtered_df will contain only the rows where 'ciq_id' is repeated less than 3 times\n",
        "\n",
        "# Output the newly filtered data to a new DataFrame\n",
        "ceo_final_v2.to_csv('ceo_final_v2.csv', index=False)\n",
        "pickle.dump(ceo_final_v2, open(\"ceo_final_v2.p\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "b4gfG216lOzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ciq_df = pd.read_csv('ciq_data_raw_20231127.csv', encoding='latin-1')\n",
        "\n",
        "ciq_df.sort_values(['ciq_id','start_year'], inplace=True)\n",
        "\n",
        "# Bring in historical values:\n",
        "ciq_df['sp_adj_7'] = ciq_df.groupby(['ciq_id'])['sp_adj'].transform(lambda x: x.shift(7))\n",
        "ciq_df['sp_adj_2'] = ciq_df.groupby(['ciq_id'])['sp_adj'].transform(lambda x: x.shift(2))\n",
        "ciq_df['sp_adj_1x'] = ciq_df.groupby(['ciq_id'])['sp_adj'].transform(lambda x: x.shift(-1))\n",
        "ciq_df['sp_adj_5x'] = ciq_df.groupby(['ciq_id'])['sp_adj'].transform(lambda x: x.shift(-5))\n",
        "\n",
        "ciq_df['sales_7'] = ciq_df.groupby(['ciq_id'])['sales'].transform(lambda x: x.shift(7))\n",
        "ciq_df['sales_2'] = ciq_df.groupby(['ciq_id'])['sales'].transform(lambda x: x.shift(2))\n",
        "ciq_df['sales_1x'] = ciq_df.groupby(['ciq_id'])['sales'].transform(lambda x: x.shift(-1))\n",
        "ciq_df['sales_5x'] = ciq_df.groupby(['ciq_id'])['sales'].transform(lambda x: x.shift(-5))\n",
        "\n",
        "# Convert the necessary columns to numeric data type\n",
        "cols_to_convert = ['sp_adj', 'sp_adj_7', 'sp_adj_2', 'sp_adj_1x', 'sp_adj_5x', 'sales', 'sales_7', 'sales_2', 'sales_1x', 'sales_5x']\n",
        "for col in cols_to_convert:\n",
        "    ciq_df[col] = pd.to_numeric(ciq_df[col], errors='coerce')\n",
        "\n",
        "# Optionally fill NaN values with a default value (e.g., 0)\n",
        "#ciq_df.fillna(0, inplace=True)\n",
        "\n",
        "# Calculate growth rates:\n",
        "ciq_df['tsr_5p'] = np.power((ciq_df['sp_adj_2']/ciq_df['sp_adj_7']), .2)-1\n",
        "ciq_df['tsr_2'] = np.power((ciq_df['sp_adj']/ciq_df['sp_adj_2']), .5)-1\n",
        "ciq_df['tsr_1x'] = np.power((ciq_df['sp_adj_1x']/ciq_df['sp_adj']), 1)-1\n",
        "ciq_df['tsr_5x'] = np.power((ciq_df['sp_adj_5x']/ciq_df['sp_adj']), .2)-1\n",
        "\n",
        "ciq_df['growth_5p'] = np.power((ciq_df['sales_2']/ciq_df['sales_7']), .2)-1\n",
        "ciq_df['growth_2'] = np.power((ciq_df['sales']/ciq_df['sales_2']), .5)-1\n",
        "ciq_df['growth_1x'] = np.power((ciq_df['sales_1x']/ciq_df['sales']), 1)-1\n",
        "ciq_df['growth_5x'] = np.power((ciq_df['sales_5x']/ciq_df['sales']), .2)-1\n",
        "\n",
        "ciq_df.head(25)\n",
        "\n",
        "# Add variables for analyzing turnaround components\n",
        "ciq_df['ebitda_5p'] = ciq_df.groupby(['ciq_id'])['ebitda'].transform(lambda x: x.shift(7))\n",
        "ciq_df['ebitda_2'] = ciq_df.groupby(['ciq_id'])['ebitda'].transform(lambda x: x.shift(2))\n",
        "ciq_df['ebitda_1x'] = ciq_df.groupby(['ciq_id'])['ebitda'].transform(lambda x: x.shift(-1))\n",
        "ciq_df['ebitda_5x'] = ciq_df.groupby(['ciq_id'])['ebitda'].transform(lambda x: x.shift(-5))\n",
        "\n",
        "ciq_df['eps_5p'] = ciq_df.groupby(['ciq_id'])['eps'].transform(lambda x: x.shift(7))\n",
        "ciq_df['eps_2'] = ciq_df.groupby(['ciq_id'])['eps'].transform(lambda x: x.shift(2))\n",
        "ciq_df['eps_1x'] = ciq_df.groupby(['ciq_id'])['eps'].transform(lambda x: x.shift(-1))\n",
        "ciq_df['eps_5x'] = ciq_df.groupby(['ciq_id'])['eps'].transform(lambda x: x.shift(-5))\n",
        "\n",
        "ciq_df['sp_5p'] = ciq_df.groupby(['ciq_id'])['sp'].transform(lambda x: x.shift(7))\n",
        "ciq_df['sp_2'] = ciq_df.groupby(['ciq_id'])['sp'].transform(lambda x: x.shift(2))\n",
        "ciq_df['sp_1x'] = ciq_df.groupby(['ciq_id'])['sp'].transform(lambda x: x.shift(-1))\n",
        "ciq_df['sp_5x'] = ciq_df.groupby(['ciq_id'])['sp'].transform(lambda x: x.shift(-5))\n",
        "\n",
        "\n",
        "ciq_df['sales_2x'] = ciq_df.groupby(['ciq_id'])['sales'].transform(lambda x: x.shift(-2))\n",
        "ciq_df['capex_1x'] = ciq_df.groupby(['ciq_id'])['capex'].transform(lambda x: x.shift(-1))\n",
        "ciq_df['capex_2x'] = ciq_df.groupby(['ciq_id'])['capex'].transform(lambda x: x.shift(-2))\n",
        "ciq_df['r&d_1x'] = ciq_df.groupby(['ciq_id'])['r&d'].transform(lambda x: x.shift(-1))\n",
        "ciq_df['r&d_2x'] = ciq_df.groupby(['ciq_id'])['r&d'].transform(lambda x: x.shift(-2))\n",
        "\n",
        "# Convert the necessary columns to numeric data type\n",
        "cols_to_convert = ['sp_adj', 'sp_adj_7', 'sp_adj_2', 'sp_adj_1x', 'sp_adj_5x',\n",
        "                   'sales', 'sales_7', 'sales_2', 'sales_1x', 'sales_5x',\n",
        "                   'ebitda', 'ebitda_5p', 'ebitda_2', 'ebitda_1x', 'ebitda_5x',\n",
        "                   'eps', 'eps_5p', 'eps_2', 'eps_1x', 'eps_5x',\n",
        "                   'sp', 'sp_5p', 'sp_2', 'sp_1x', 'sp_5x',\n",
        "                   'capex', 'capex_1x', 'capex_2x',\n",
        "                   'r&d', 'r&d_1x', 'r&d_2x']\n",
        "for col in cols_to_convert:\n",
        "    ciq_df[col] = pd.to_numeric(ciq_df[col], errors='coerce')\n",
        "\n",
        "# Optionally fill NaN values with a default value (e.g., 0)\n",
        "#ciq_df.fillna(0, inplace=True)\n",
        "\n",
        "ciq_df['capex_sales'] = -1*ciq_df['capex']/ciq_df['sales']\n",
        "ciq_df['capex_sales_1x'] = -1*ciq_df['capex_1x']/ciq_df['sales_1x']\n",
        "ciq_df['capex_sales_2x'] = -1*ciq_df['capex_2x']/ciq_df['sales_2x']\n",
        "ciq_df['capex_sales_avg'] = (ciq_df['capex_sales'] + ciq_df['capex_sales_1x'] + ciq_df['capex_sales_2x'])/3.0\n",
        "\n",
        "ciq_df['r&d_sales'] = ciq_df['r&d']/ciq_df['sales']\n",
        "ciq_df['r&d_sales_1x'] = ciq_df['r&d_1x']/ciq_df['sales_1x']\n",
        "ciq_df['r&d_sales_2x'] = ciq_df['r&d_2x']/ciq_df['sales_2x']\n",
        "ciq_df['r&d_sales_avg'] = (ciq_df['r&d_sales'] + ciq_df['r&d_sales_1x'] + ciq_df['r&d_sales_2x'])/3.0\n",
        "\n",
        "ciq_df['restructuring_1x'] = ciq_df.groupby(['ciq_id'])['restructuring'].transform(lambda x: x.shift(-1))\n",
        "ciq_df['tf_1x'] = ciq_df.groupby(['ciq_id'])['tf'].transform(lambda x: x.shift(-1))\n",
        "#ciq_df.rename(columns={'restructuring':'restructuring_1x'}, inplace=True)\n",
        "ciq_df.head(20)\n",
        "\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "ciq_df.replace(np.inf, np.NaN, inplace=True)\n",
        "#ciq_df.head()\n",
        "ciq_df.to_csv('ciq_data_20231127.csv', index=False)\n",
        "pickle.dump(ciq_df, open(\"ciq_data_20231127.p\", \"wb\" ))\n",
        "\n",
        "# Utilizing the describe function to provide a statistical summary of the DataFrame, which can be useful for analysis.\n",
        "ciq_df.describe()"
      ],
      "metadata": {
        "id": "U5itOfQUlQUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "#!pip install pycountry\n",
        "#import pycountry\n",
        "\n",
        "# Load CEO data\n",
        "ceo_final = pickle.load(open(\"ceo_final_v2.p\", \"rb\" ))\n",
        "\n",
        "# Load CIQ data\n",
        "ciq_df = pickle.load(open(\"ciq_data_20231127.p\", \"rb\" ))\n",
        "\n",
        "# Load industry sector data\n",
        "helper_2 = pd.read_csv('231120_company_id.csv')[['ciq_id','industry_sector']]\n",
        "helper_2.rename(columns={'industry_sector':'sector'}, inplace=True)\n",
        "\n",
        "# Compute 'start_year_int' by converting 'start_year' to numeric, converting errors to NaN, and then filling NaN with 1000\n",
        "ceo_final['start_year_int'] = ceo_final['start_year'].apply(lambda x: pd.to_numeric(x, errors='ignore')).fillna(1000)\n",
        "\n",
        "# Same conversion for 'end_year_int'\n",
        "ceo_final['end_year_int'] = ceo_final['end_year'].apply(lambda x: pd.to_numeric(x, errors='ignore')).fillna(3000)\n",
        "\n",
        "# Identify interim CEOs where 'end_year_int' is equal to 'start_year_int'\n",
        "ceo_final['interim'] = np.where((ceo_final['end_year_int'] == ceo_final['start_year_int']), 1, 0)\n",
        "\n",
        "duplicate_count = ceo_final.duplicated(subset='ciq_id', keep=False).sum()\n",
        "print(f'Count of duplicate entries based on ciq_id: {duplicate_count}')\n",
        "\n",
        "ceo_final.describe()\n",
        "\n",
        "# Filter out interim CEOs\n",
        "ceo_final = ceo_final[ceo_final['interim'] == 0].copy()  # Use copy to avoid SettingWithCopyWarning\n",
        "\n",
        "# Drop unnecessary columns\n",
        "ceo_final.drop(['start_year_int', 'end_year_int', 'interim'], axis=1, inplace=True)\n",
        "\n",
        "ceo_final.describe()\n",
        "\n",
        "#ceo_final_premerge = ceo_final.to_csv('ceo_final_premerge.csv', index=False)\n",
        "\n",
        "\n",
        "# First Merge\n",
        "merged_df = ceo_final.merge(ciq_df, on=['ciq_id', 'start_year'], how='right')\n",
        "# Before the first merge\n",
        "print(\"\\After first merge:\")\n",
        "print(merged_df['start_year'].value_counts())\n",
        "\n",
        "#ceo_final_merge = merged_df.to_csv('ceo_final_merge.csv', index=False)\n",
        "\n",
        "# Second Merge\n",
        "merged_df = merged_df.merge(helper_2, on='ciq_id', how='right')\n",
        "print(\"\\nAfter third merge:\")\n",
        "print(merged_df['start_year'].value_counts())\n",
        "\n",
        "#ceo_final_merge2 = merged_df.to_csv('ceo_final_merge2.csv', index=False)\n",
        "\n",
        "# Save Output\n",
        "ceo_final = merged_df\n",
        "ceo_final.to_csv('ceo_final.csv', index=False)\n",
        "pickle.dump(merged_df, open(\"ceo_final.p\", \"wb\" ))"
      ],
      "metadata": {
        "id": "84JOodrulRsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load industry data from a CSV file into a DataFrame\n",
        "ind_df = pd.read_csv(\"industry_data_20231127.csv\")\n",
        "# Extract the sector information from the 'industry' column and create a new column 'sector'\n",
        "ind_df['sector'] = ind_df['industry'].map(lambda x: x[8:-9])\n",
        "# Rename the sector 'Health Care' to 'Healthcare' for consistency\n",
        "#ind_df.loc[ind_df.sector==\"Health Care\", 'sector'] = \"Healthcare\"\n",
        "ind_df.loc[ind_df.sector==\"Telecommunication Services\", 'sector'] = \"Communication Services\"\n",
        "\n",
        "# Drop unwanted columns from the 'ind_df' DataFrame\n",
        "ind_df.drop(['industry', 'index', 'date', 'ciq total return gross', 'check industry', 'sp', 'growth'], axis=1, inplace=True)\n",
        "# Rename the remaining columns for clarity and consistency\n",
        "ind_df.rename(columns={'total return':'ind_sp', 'year':'start_year', 'ciq':'ciq_id'}, inplace=True)\n",
        "# Filter out rows where 'ind_sp' is 0 as these are likely to be invalid entries\n",
        "ind_df = ind_df[ind_df.ind_sp != 0]\n",
        "\n",
        "# Calculate the historical total shareholder return (TSR) values for each industry\n",
        "# by shifting the 'ind_sp' values by different time periods (7, 2, -1, and -5 years)\n",
        "ind_df['ind_sp_5p'] = ind_df.groupby(['ciq_id'])['ind_sp'].transform(lambda x: x.shift(7))\n",
        "ind_df['ind_sp_2'] = ind_df.groupby(['ciq_id'])['ind_sp'].transform(lambda x: x.shift(2))\n",
        "ind_df['ind_sp_1x'] = ind_df.groupby(['ciq_id'])['ind_sp'].transform(lambda x: x.shift(-1))\n",
        "ind_df['ind_sp_5x'] = ind_df.groupby(['ciq_id'])['ind_sp'].transform(lambda x: x.shift(-5))\n",
        "\n",
        "# Calculate the TSR growth rates for each industry over different time periods (5-year, 2-year, 1-year forward, and 5-year forward)\n",
        "ind_df['ind_tsr_5p'] = np.power((ind_df['ind_sp_2']/ind_df['ind_sp_5p']), .2)-1\n",
        "ind_df['ind_tsr_2'] = np.power((ind_df['ind_sp']/ind_df['ind_sp_2']), .5)-1\n",
        "ind_df['ind_tsr_1x'] = np.power((ind_df['ind_sp_1x']/ind_df['ind_sp']), 1)-1\n",
        "ind_df['ind_tsr_5x'] = np.power((ind_df['ind_sp_5x']/ind_df['ind_sp']), .2)-1\n",
        "\n",
        "# Display the first 20 rows of the 'ind_df' DataFrame to verify the calculations\n",
        "ind_df.head(20)\n",
        "\n",
        "# Merge the industry TSR growth rates with the 'ceo_full' DataFrame on the 'start_year' and 'sector' columns\n",
        "ceo_final = ceo_final.merge(ind_df[['start_year', 'sector', 'ind_tsr_2', 'ind_tsr_1x', 'ind_tsr_5x', 'ind_tsr_5p']], on=['start_year', 'sector'], how='left')\n",
        "# Uncomment the following line to display the first 5 rows of the 'ceo_full' DataFrame to verify the merge\n",
        "#ceo_full.head()\n",
        "# Uncomment the following line to display the column names of the 'ind_df' DataFrame\n",
        "#ind_df.columns\n",
        "\n",
        "# Calculate the difference between the company TSR and industry TSR for different time periods\n",
        "ceo_final['tsr_5p_delta'] = ceo_final['tsr_5p'] - ceo_final['ind_tsr_5p']\n",
        "ceo_final['tsr_2_delta'] = ceo_final['tsr_2'] - ceo_final['ind_tsr_2']\n",
        "ceo_final['tsr_1x_delta'] = ceo_final['tsr_1x'] - ceo_final['ind_tsr_1x']\n",
        "ceo_final['tsr_5x_delta'] = ceo_final['tsr_5x'] - ceo_final['ind_tsr_5x']\n",
        "\n",
        "# Display the first 20 rows of the 'ceo_full' DataFrame to verify the calculations\n",
        "ceo_final.head(20)\n",
        "\n",
        "# Calculate the change in TSR growth (vs prior period) for different time periods\n",
        "ceo_final['tsr_2_chg'] = ceo_final['tsr_2_delta'] - ceo_final['tsr_5p_delta']\n",
        "ceo_final['tsr_1x_chg'] = ceo_final['tsr_1x_delta'] - ceo_final['tsr_2_delta']\n",
        "ceo_final['tsr_5x_chg'] = ceo_final['tsr_5x_delta'] - ceo_final['tsr_2_delta']\n",
        "\n",
        "# Create bins and labels for categorizing the change in TSR growth\n",
        "# The bins are defined by the range of TSR growth rates, and the labels represent different categories of growth\n",
        "bins = [-999]\n",
        "lbls = [\"--\"]\n",
        "for i in range(-50,55,5):\n",
        "    bins.append(float(i)/100.0)\n",
        "    lbls.append(str(i))\n",
        "lbls.remove(\"0\")\n",
        "bins.append(999)\n",
        "lbls.append(\"++\")\n",
        "\n",
        "# Create new columns to categorize the change in TSR growth for different time periods\n",
        "ceo_final['2_chg_lbl'] = pd.cut(ceo_final['tsr_2_chg'], bins, labels=lbls)\n",
        "ceo_final['1x_chg_lbl'] = pd.cut(ceo_final['tsr_1x_chg'], bins, labels=lbls)\n",
        "ceo_final['5x_chg_lbl'] = pd.cut(ceo_final['tsr_5x_chg'], bins, labels=lbls)\n",
        "\n",
        "# Create big bins for color-coding the change in TSR growth\n",
        "bins_2 = [-999,-.1,0,.1,999]\n",
        "lbls_2 = [\"V_neg\",\"neg\",\"pos\",\"V_pos\"]\n",
        "\n",
        "# Create new columns to categorize the change in TSR growth using big bins for color-coding\n",
        "ceo_final['cat_2'] = pd.cut(ceo_final['tsr_2_chg'], bins_2, labels=lbls_2)\n",
        "ceo_final['cat_1x'] = pd.cut(ceo_final['tsr_1x_chg'], bins_2, labels=lbls_2)\n",
        "ceo_final['cat_5x'] = pd.cut(ceo_final['tsr_5x_chg'], bins_2, labels=lbls_2)\n",
        "\n",
        "ceo_final.describe()"
      ],
      "metadata": {
        "id": "XWrwffJTlWJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new columns to calculate the growth rates of EBITDA, EPS, and SP for different time periods\n",
        "ceo_final['ebitda_2_growth'] = np.power((ceo_final['ebitda']/ceo_final['ebitda_2']), .5) - 1\n",
        "ceo_final['eps_2_growth'] = np.power((ceo_final['eps']/ceo_final['eps_2']), .5) - 1\n",
        "ceo_final['sp_2_growth'] = np.power((ceo_final['sp']/ceo_final['sp_2']), .5) - 1\n",
        "\n",
        "ceo_final['ebitda_1x_growth'] = ceo_final['ebitda_1x']/ceo_final['ebitda'] - 1\n",
        "ceo_final['eps_1x_growth'] = ceo_final['eps_1x']/ceo_final['eps'] - 1\n",
        "ceo_final['sp_1x_growth'] = ceo_final['sp_1x']/ceo_final['sp'] - 1\n",
        "\n",
        "ceo_final['ebitda_5x_growth'] = np.power((ceo_final['ebitda_5x']/ceo_final['ebitda']), .2) - 1\n",
        "ceo_final['eps_5x_growth'] = np.power((ceo_final['eps_5x']/ceo_final['eps']), .2) - 1\n",
        "ceo_final['sp_5x_growth'] = np.power((ceo_final['sp_5x']/ceo_final['sp']), .2) - 1\n",
        "\n",
        "# Calculate the contribution of top-line sales growth, cost control (earnings growth),\n",
        "# cash management (cash growth), and market expectations (price growth) to TSR\n",
        "# The calculations are done for different time periods (2-year, 1-year forward, and 5-year forward)\n",
        "# to analyze the turnaround components\n",
        "\n",
        "# * 2 *\n",
        "# For the 2-year period\n",
        "ceo_final['sales_2_contrib'] = ceo_final['growth_2']\n",
        "ceo_final['earnings_2_contrib'] = ceo_final['eps_2_growth'] - ceo_final['sales_2_contrib']\n",
        "ceo_final['cash_2_contrib'] = ceo_final['tsr_2'] - ceo_final['sp_2_growth']\n",
        "\n",
        "# Corrections:\n",
        "# If eps is blank, use ebitda\n",
        "ceo_final['earnings_2_contrib'] = np.where((np.isnan(ceo_final['earnings_2_contrib'])), ceo_final['ebitda_2_growth'] - ceo_final['sales_2_contrib'], ceo_final['earnings_2_contrib'])\n",
        "\n",
        "# If earnings go from negative to positive, set contribution = total TSR growth (vice versa):\n",
        "ceo_final['earnings_2_contrib'] = np.where(((ceo_final['eps_2'] < 0) & (ceo_final['eps'] > 0)), ceo_final['tsr_2'], ceo_final['earnings_2_contrib'])\n",
        "ceo_final['earnings_2_contrib'] = np.where(((ceo_final['eps_2'] > 0) & (ceo_final['eps'] < 0)), ceo_final['tsr_2'], ceo_final['earnings_2_contrib'])\n",
        "\n",
        "# If earnings are still negative, invert growth rate:\n",
        "ceo_final['earnings_2_contrib'] = np.where((ceo_final['eps'] < 0), -1 * ceo_final['earnings_2_contrib'], ceo_final['earnings_2_contrib'])\n",
        "\n",
        "# Cap earnings contribution at 100% - arbitrary but need to cut off the 88x growth contribution\n",
        "ceo_final['earnings_2_contrib'] = np.where((ceo_final['earnings_2_contrib'] < -1), -1, ceo_final['earnings_2_contrib'])\n",
        "ceo_final['earnings_2_contrib'] = np.where((ceo_final['earnings_2_contrib'] > 1), 1, ceo_final['earnings_2_contrib'])\n",
        "\n",
        "ceo_final['price_2_contrib'] = ceo_final['tsr_2'] - ceo_final['sales_2_contrib'] - ceo_final['earnings_2_contrib'] - ceo_final['cash_2_contrib']\n",
        "\n",
        "# * 1x *\n",
        "\n",
        "# Decomposing TSR for a 1-year period\n",
        "ceo_final['sales_1x_contrib'] = ceo_final['growth_1x']\n",
        "ceo_final['earnings_1x_contrib'] = ceo_final['eps_1x_growth'] - ceo_final['sales_1x_contrib']\n",
        "ceo_final['cash_1x_contrib'] = ceo_final['tsr_1x'] - ceo_final['sp_1x_growth']\n",
        "\n",
        "# Corrections:\n",
        "# If eps is blank, use ebitda\n",
        "ceo_final['earnings_1x_contrib'] = np.where((np.isnan(ceo_final['earnings_1x_contrib'])), ceo_final['ebitda_1x_growth'] - ceo_final['sales_1x_contrib'], ceo_final['earnings_1x_contrib'])\n",
        "\n",
        "# If earnings go from negative to positive, set contribution = total TSR growth:\n",
        "ceo_final['earnings_1x_contrib'] = np.where(((ceo_final['eps'] < 0) & (ceo_final['eps_1x'] > 0)), ceo_final['tsr_1x'], ceo_final['earnings_1x_contrib'])\n",
        "ceo_final['earnings_1x_contrib'] = np.where(((ceo_final['eps'] > 0) & (ceo_final['eps_1x'] < 0)), ceo_final['tsr_1x'], ceo_final['earnings_1x_contrib'])\n",
        "\n",
        "# If earnings are still negative, invert growth rate:\n",
        "ceo_final['earnings_1x_contrib'] = np.where((ceo_final['eps_1x'] < 0), -1 * ceo_final['earnings_1x_contrib'], ceo_final['earnings_1x_contrib'])\n",
        "\n",
        "# Cap earnings contribution at 100% - arbitrary but need to cut off the 88x growth contribution\n",
        "ceo_final['earnings_1x_contrib'] = np.where((ceo_final['earnings_1x_contrib'] < -1), -1, ceo_final['earnings_1x_contrib'])\n",
        "ceo_final['earnings_1x_contrib'] = np.where((ceo_final['earnings_1x_contrib'] > 1), 1, ceo_final['earnings_1x_contrib'])\n",
        "\n",
        "ceo_final['price_1x_contrib'] = ceo_final['tsr_1x'] - ceo_final['sales_1x_contrib'] - ceo_final['earnings_1x_contrib'] - ceo_final['cash_1x_contrib']\n",
        "\n",
        "# * 5x *\n",
        "\n",
        "# Decomposing TSR for a 5-year period\n",
        "ceo_final['sales_5x_contrib'] = ceo_final['growth_5x']\n",
        "ceo_final['earnings_5x_contrib'] = ceo_final['eps_5x_growth'] - ceo_final['sales_5x_contrib']\n",
        "ceo_final['cash_5x_contrib'] = ceo_final['tsr_5x'] - ceo_final['sp_5x_growth']\n",
        "\n",
        "# Corrections:\n",
        "# If eps is blank, use ebitda\n",
        "ceo_final['earnings_5x_contrib'] = np.where((np.isnan(ceo_final['earnings_5x_contrib'])), ceo_final['ebitda_5x_growth'] - ceo_final['sales_5x_contrib'], ceo_final['earnings_5x_contrib'])\n",
        "\n",
        "# If earnings go from negative to positive, set contribution = total TSR growth:\n",
        "ceo_final['earnings_5x_contrib'] = np.where(((ceo_final['eps'] < 0) & (ceo_final['eps_5x'] > 0)), ceo_final['tsr_5x'], ceo_final['earnings_5x_contrib'])\n",
        "ceo_final['earnings_5x_contrib'] = np.where(((ceo_final['eps'] > 0) & (ceo_final['eps_5x'] < 0)), ceo_final['tsr_5x'], ceo_final['earnings_5x_contrib'])\n",
        "\n",
        "# If earnings are still negative, invert growth rate:\n",
        "ceo_final['earnings_5x_contrib'] = np.where((ceo_final['eps_5x'] < 0), -1 * ceo_final['earnings_5x_contrib'], ceo_final['earnings_5x_contrib'])\n",
        "\n",
        "# Cap earnings contribution at 100% - arbitrary but need to cut off the 88x growth contribution\n",
        "ceo_final['earnings_5x_contrib'] = np.where((ceo_final['earnings_5x_contrib'] < -1), -1, ceo_final['earnings_5x_contrib'])\n",
        "ceo_final['earnings_5x_contrib'] = np.where((ceo_final['earnings_5x_contrib'] > 1), 1, ceo_final['earnings_5x_contrib'])\n",
        "\n",
        "ceo_final['price_5x_contrib'] = ceo_final['tsr_5x'] - ceo_final['sales_5x_contrib'] - ceo_final['earnings_5x_contrib'] - ceo_final['cash_5x_contrib']\n",
        "\n",
        "# Utilizing the describe function to provide a statistical summary of the DataFrame, which can be useful for analysis.\n",
        "ceo_final.describe()"
      ],
      "metadata": {
        "id": "ccUCZ473lWu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the data\n",
        "df_nlp = pd.read_csv(\"231127_nlp.csv\")\n",
        "\n",
        "# Step 2: Impute 2022 Data\n",
        "df_nlp_2022 = df_nlp[df_nlp['year'] == 2021].copy()\n",
        "df_nlp_2022['year'] = 2022\n",
        "data = pd.concat([df_nlp, df_nlp_2022], axis=0)\n",
        "\n",
        "# Step 3: Impute for Companies with No Values\n",
        "industry_avg = data.groupby(['industry', 'year']).mean().reset_index()\n",
        "\n",
        "def fill_with_industry_avg(row):\n",
        "    if pd.isnull(row['biological']):\n",
        "        row['biological'] = industry_avg[(industry_avg['industry'] == row['industry']) & (industry_avg['year'] == row['year'])]['biological'].values[0]\n",
        "    if pd.isnull(row['longterm']):\n",
        "        row['longterm'] = industry_avg[(industry_avg['industry'] == row['industry']) & (industry_avg['year'] == row['year'])]['longterm'].values[0]\n",
        "    return row\n",
        "\n",
        "df_nlp = data.apply(fill_with_industry_avg, axis=1)\n",
        "\n",
        "# Step 4: Impute Values for Companies with Partial Data\n",
        "def fill_with_company_avg(group):\n",
        "    avg_biological = group['biological'].mean()\n",
        "    avg_longterm = group['longterm'].mean()\n",
        "    group['biological'].fillna(avg_biological, inplace=True)\n",
        "    group['longterm'].fillna(avg_longterm, inplace=True)\n",
        "    return group\n",
        "\n",
        "df_nlp = df_nlp.groupby('name').apply(fill_with_company_avg)\n",
        "\n",
        "# Step 5: Save the Data\n",
        "df_nlp.to_csv(\"231127_nlp_clean.csv\", index=False)\n",
        "pickle.dump(df_nlp, open(\"231121_nlp_clean.p\", \"wb\" ))\n",
        "\n",
        "# Filter the data to include only the MD&A section (section 56), and then calculate the mean NLP scores per company per year.\n",
        "df_nlp = df_nlp.groupby([\"ciq_id\", \"year\"])[['biological', 'longterm', 'purpose']].mean().reset_index()\n",
        "\n",
        "# Adjust the year to account for the fact that 10-Ks are published in the year following the year they report on.\n",
        "df_nlp[\"year\"] = df_nlp[\"year\"] - 1\n",
        "\n",
        "# Calculate net scores for 'longterm' and 'biological' by subtracting other relevant scores. This provides a measure of the net orientation of each firm's strategy.\n",
        "df_nlp[\"net_longterm\"] = df_nlp[\"longterm\"] - df_nlp[\"biological\"]\n",
        "\n",
        "# Standardize the net scores by year to have a mean of 0 and a standard deviation of 1, making them comparable across years.\n",
        "nlp_scores = ['net_longterm']\n",
        "df_nlp_std = df_nlp.copy()\n",
        "for var in nlp_scores:\n",
        "    df_nlp_std[var] = df_nlp_std.groupby(\"year\")[var].transform(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "# Select only the columns needed for further analysis.\n",
        "df_nlp_std = df_nlp_std[['ciq_id','year','net_longterm']]\n",
        "\n",
        "# Display summary statistics of the standardized net scores to understand their distribution.\n",
        "df_nlp_std.describe()\n",
        "\n",
        "# Rename the 'year' column to 'start_year' to match the naming convention in other DataFrames.\n",
        "df_nlp_std.rename(columns={'year':'start_year'}, inplace=True)\n",
        "\n",
        "# Merge the standardized net scores into the main DataFrame 'ceo_final_v2'.\n",
        "ceo_final_v2 = ceo_final.merge(df_nlp_std, how='left', on=['ciq_id','start_year']).sort_values(['ciq_id','start_year'])\n",
        "#ceo_final_v2.head(20)\n",
        "\n",
        "# Shift the standardized net scores by one year forward and two years forward to create lagged variables.\n",
        "# This is done to analyze the effect of past BHI strategy scores on later financial performance.\n",
        "ceo_final_v2['net_longterm_1x'] = ceo_final_v2.groupby(['ciq_id'])['net_longterm'].transform(lambda x: x.shift(-1))\n",
        "ceo_final_v2['net_longterm_2x'] = ceo_final_v2.groupby(['ciq_id'])['net_longterm'].transform(lambda x: x.shift(-2))\n",
        "\n",
        "# Calculate the 3-year average of the standardized net longterm scores.\n",
        "ceo_final_v2['net_longterm_avg'] = (ceo_final_v2['net_longterm'] + ceo_final_v2['net_longterm_1x'] + ceo_final_v2['net_longterm_2x']) / 3.0\n",
        "\n",
        "# Similar operations are performed to create lagged variables for the CEO name and title.\n",
        "ceo_final_v2['ceo_1x'] = ceo_final_v2.groupby(['ciq_id'])['ceo'].transform(lambda x: x.shift(-1))\n",
        "ceo_final_v2['ceo_title_1x'] = ceo_final_v2.groupby(['ciq_id'])['prev_title'].transform(lambda x: x.shift(-1))\n",
        "####UNCLEAR HOW THIS WAS DONE BEFORE; MAKING ASSUMPTION\n",
        "ceo_final_v2['name_x'] = ceo_final_v2.groupby(['ciq_id'])['ceo'].transform(lambda x: x.shift(-1))\n",
        "\n",
        "ceo_final_v2 = ceo_final_v2\n",
        "ceo_final_v2.to_csv('ceo_final_v2.csv', index=False)"
      ],
      "metadata": {
        "id": "E4UNQGvplX5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save old version as df_all\n",
        "df_all = ceo_final_v2\n",
        "df_all.to_csv('df_all.csv', index=False)\n",
        "\n",
        "# Remove if max market cap < $5B\n",
        "#ceo_final_v2['max_mc'] = ceo_final_v2.groupby('ciq_id')['mc'].transform(lambda x: x.max())\n",
        "#ceo_final_v2 = ceo_final_v2[ceo_final_v2['max_mc']>=5000]\n",
        "#ceo_final_v2.drop('max_mc',axis=1, inplace=True)\n",
        "\n",
        "#ceo_final_v2.describe()\n",
        "\n",
        "\n",
        "# merge in company name\n",
        "helper_df = pd.read_csv('231120_company_id.csv')[['ciq_id','name']]\n",
        "\n",
        "ceo_final_v2 = ceo_final_v2.merge(helper_df, how='left', on='ciq_id')\n",
        "ceo_final_v2.describe(include='all')\n",
        "\n",
        "'''\n",
        "# # TEMPORARY\n",
        "# exclude mutual funds, etc.\n",
        "fund_list = pd.read_csv('fund_list.csv', index_col=False)[['ciq','fund']]\n",
        "fund_list.rename(columns={'ciq': 'ciq_id'}, inplace=True)\n",
        "ceo_temp = ceo_final_v2.merge(fund_list, how='left', on='ciq_id')\n",
        "\n",
        "ceo_temp = ceo_temp[ceo_temp['fund']!='Y'].drop('fund',axis=1)\n",
        "ceo_temp.describe()'''\n",
        "\n",
        "# export\n",
        "#ceo_temp.to_csv('output_20170814_all.csv', index=False)\n",
        "ceo_final_v2.to_csv('output_20231127.csv', index=False)\n",
        "pickle.dump(ceo_final, open(\"output_20231127.p\", \"wb\" ))"
      ],
      "metadata": {
        "id": "CUOTQwHklZMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics for the DataFrame to understand the overall distribution of the data.\n",
        "df_all.describe(include='all')\n",
        "\n",
        "# Convert columns to numeric data type, if necessary\n",
        "df_all[['mc','capex', 'sales', 'r&d']] = df_all[['mc','capex', 'sales', 'r&d']].apply(pd.to_numeric, errors='coerce')\n",
        "# Replace non-numeric values with NaN or 0, as appropriate\n",
        "df_all[['mc','capex', 'sales', 'r&d']] = df_all[['mc','capex', 'sales', 'r&d']].replace('--', np.nan).fillna(0)\n",
        "\n",
        "\n",
        "# Calculate the total earnings and cash-adjusted market cap for each company.\n",
        "# Calculating shares outstanding by dividing market cap by stock price.\n",
        "df_all['shares'] = df_all['mc'] / df_all['sp']\n",
        "# Calculating total earnings by multiplying earnings per share by shares outstanding.\n",
        "df_all['earnings_tot'] = df_all['eps'] * df_all['shares']\n",
        "# Setting negative earnings to zero as they are considered non-meaningful for this analysis.\n",
        "df_all['earnings_tot'] = np.where(df_all['earnings_tot'] < 0, 0, df_all['earnings_tot'])\n",
        "# Calculating cash-adjusted market cap by multiplying adjusted stock price by shares outstanding.\n",
        "df_all['mc_adj'] = df_all['sp_adj'] * df_all['shares']\n",
        "# Filling any missing values with zero in the 'shares', 'earnings_tot', and 'mc_adj' columns.\n",
        "df_all[['shares','earnings_tot','mc_adj']] = df_all[['shares','earnings_tot','mc_adj']].fillna(value=0)\n",
        "\n",
        "# Calculate industry aggregates to understand the overall trends in the data.\n",
        "# Summing up capital expenditures, sales, earnings, market cap, and cash-adjusted market cap for each sector in each year.\n",
        "agg_df = df_all[df_all['sp']>0].groupby([\"sector\", \"start_year\"])[['capex','sales','earnings_tot','mc','mc_adj']].sum().reset_index()\n",
        "# Merging the industry TSR data into the industry aggregate DataFrame.\n",
        "agg_df = agg_df.merge(df_all.groupby(['sector','start_year'])[['ind_tsr_5p','ind_tsr_2','ind_tsr_1x','ind_tsr_5x']].first().reset_index(), how='left',on=['sector','start_year'])\n",
        "\n",
        "agg_df.head(10)\n",
        "\n",
        "# Create lagged variables for the industry-adjusted market cap to analyze the growth over different periods.\n",
        "agg_df['ind_mc_adj_5p'] = agg_df.groupby(['sector'])['mc_adj'].transform(lambda x: x.shift(7))\n",
        "agg_df['ind_mc_adj_2'] = agg_df.groupby(['sector'])['mc_adj'].transform(lambda x: x.shift(2))\n",
        "agg_df['ind_mc_adj_1x'] = agg_df.groupby(['sector'])['mc_adj'].transform(lambda x: x.shift(-1))\n",
        "agg_df['ind_mc_adj_5x'] = agg_df.groupby(['sector'])['mc_adj'].transform(lambda x: x.shift(-5))\n",
        "\n",
        "# Rename the column for better readability.\n",
        "agg_df.rename(columns={'mc_adj':'ind_mc_adj'}, inplace=True)\n",
        "\n",
        "# Calculate implied industry TSR (Total Shareholder Return) based on the growth of the industry-adjusted market cap.\n",
        "agg_df['ind_tsr_implied_5p'] = np.power((agg_df['ind_mc_adj_2']/agg_df['ind_mc_adj_5p']), .2)-1\n",
        "agg_df['ind_tsr_implied_2'] = np.power((agg_df['ind_mc_adj']/agg_df['ind_mc_adj_2']), .5)-1\n",
        "agg_df['ind_tsr_implied_1x'] = np.power((agg_df['ind_mc_adj_1x']/agg_df['ind_mc_adj']), 1)-1\n",
        "agg_df['ind_tsr_implied_5x'] = np.power((agg_df['ind_mc_adj_5x']/agg_df['ind_mc_adj']), .2)-1\n",
        "\n",
        "# Calculate the change in industry TSR over different periods.\n",
        "agg_df['ind_tsr_chg_2'] = agg_df['ind_tsr_implied_2'] - agg_df['ind_tsr_implied_5p']\n",
        "agg_df['ind_tsr_chg_1x'] = agg_df['ind_tsr_implied_1x'] - agg_df['ind_tsr_implied_2']\n",
        "agg_df['ind_tsr_chg_5x'] = agg_df['ind_tsr_implied_5x'] - agg_df['ind_tsr_implied_2']\n",
        "\n",
        "# Create lagged variables for other financial metrics to analyze the growth over different periods.\n",
        "for i in ['sales', 'earnings_tot', 'mc']:\n",
        "    agg_df[i+'_5p'] = agg_df.groupby(['sector'])[i].transform(lambda x: x.shift(7))\n",
        "    agg_df[i+'_2'] = agg_df.groupby(['sector'])[i].transform(lambda x: x.shift(2))\n",
        "    agg_df[i+'_1x'] = agg_df.groupby(['sector'])[i].transform(lambda x: x.shift(-1))\n",
        "    agg_df[i+'_5x'] = agg_df.groupby(['sector'])[i].transform(lambda x: x.shift(-5))\n",
        "\n",
        "    # Calculate the growth rate of various financial metrics over different periods.\n",
        "    agg_df[i+'_5p_growth'] = np.power((agg_df[i]/agg_df[i+'_5p']),.2) - 1\n",
        "    agg_df[i+'_2_growth'] = np.power((agg_df[i]/agg_df[i+'_2']),.5) - 1\n",
        "    agg_df[i+'_1x_growth'] = np.power((agg_df[i+'_1x']/agg_df[i]),1) - 1\n",
        "    agg_df[i+'_5x_growth'] = np.power((agg_df[i+'_5x']/agg_df[i]),.2) - 1\n",
        "\n",
        "agg_df.describe()\n",
        "\n",
        "# Top-line contribution = sales growth %\n",
        "# Cost contribution = (earnings growth % - sales contrib.)\n",
        "# Cash contribution = (sp-adj growth - sp growth)\n",
        "# Expectations contribution = everything else (= P/E change)\n",
        "\n",
        "# Calculate the contributions of sales, earnings, cash, and price to the industry TSR for different periods.\n",
        "agg_df['ind_sales_2_contrib'] = agg_df['sales_2_growth']\n",
        "agg_df['ind_earnings_2_contrib'] = agg_df['earnings_tot_2_growth'] - agg_df['ind_sales_2_contrib']\n",
        "agg_df['ind_earnings_2_contrib'] = np.where(agg_df['ind_earnings_2_contrib']>0.5, 0.5, agg_df['ind_earnings_2_contrib'])\n",
        "agg_df['ind_earnings_2_contrib'] = np.where(agg_df['ind_earnings_2_contrib']<-0.5, -0.5, agg_df['ind_earnings_2_contrib'])\n",
        "# Cap earnings contribution at 50% - arbitrary but needed to cut off extreme values.\n",
        "agg_df['ind_cash_2_contrib'] = agg_df['ind_tsr_implied_2'] - agg_df['mc_2_growth']\n",
        "agg_df['ind_price_2_contrib'] = agg_df['ind_tsr_implied_2'] - agg_df['ind_sales_2_contrib'] - agg_df['ind_earnings_2_contrib'] - agg_df['ind_cash_2_contrib']\n",
        "\n",
        "for i in ['ind_sales_2_contrib','ind_earnings_2_contrib','ind_cash_2_contrib','ind_price_2_contrib']:\n",
        "    agg_df[i] = agg_df[i] + (agg_df['ind_tsr_2']-agg_df['ind_tsr_implied_2'])/4.0\n",
        "\n",
        "# Similar calculations are done for 1-year and 5-year periods.\n",
        "agg_df['ind_sales_1x_contrib'] = agg_df['sales_1x_growth']\n",
        "agg_df['ind_earnings_1x_contrib'] = agg_df['earnings_tot_1x_growth'] - agg_df['ind_sales_1x_contrib']\n",
        "agg_df['ind_earnings_1x_contrib'] = np.where(agg_df['ind_earnings_1x_contrib']>0.5, 0.5, agg_df['ind_earnings_1x_contrib'])\n",
        "agg_df['ind_earnings_1x_contrib'] = np.where(agg_df['ind_earnings_1x_contrib']<-0.5, -0.5, agg_df['ind_earnings_1x_contrib'])\n",
        "# Cap earnings contribution at 50% - arbitrary but needed to cut off extreme values.\n",
        "agg_df['ind_cash_1x_contrib'] = agg_df['ind_tsr_implied_1x'] - agg_df['mc_1x_growth']\n",
        "agg_df['ind_price_1x_contrib'] = agg_df['ind_tsr_implied_1x'] - agg_df['ind_sales_1x_contrib'] - agg_df['ind_earnings_1x_contrib'] - agg_df['ind_cash_1x_contrib']\n",
        "\n",
        "for i in ['ind_sales_1x_contrib','ind_earnings_1x_contrib','ind_cash_1x_contrib','ind_price_1x_contrib']:\n",
        "    agg_df[i] = agg_df[i] + (agg_df['ind_tsr_1x']-agg_df['ind_tsr_implied_1x'])/4.0\n",
        "\n",
        "agg_df['ind_sales_5x_contrib'] = agg_df['sales_5x_growth']\n",
        "agg_df['ind_earnings_5x_contrib'] = agg_df['earnings_tot_5x_growth'] - agg_df['ind_sales_5x_contrib']\n",
        "agg_df['ind_earnings_5x_contrib'] = np.where(agg_df['ind_earnings_5x_contrib']>0.5, 0.5, agg_df['ind_earnings_5x_contrib'])\n",
        "agg_df['ind_earnings_5x_contrib'] = np.where(agg_df['ind_earnings_5x_contrib']<-0.5, -0.5, agg_df['ind_earnings_5x_contrib'])\n",
        "# Cap earnings contribution at 50% - arbitrary but needed to cut off extreme values.\n",
        "agg_df['ind_cash_5x_contrib'] = agg_df['ind_tsr_implied_5x'] - agg_df['mc_5x_growth']\n",
        "agg_df['ind_price_5x_contrib'] = agg_df['ind_tsr_implied_5x'] - agg_df['ind_sales_5x_contrib'] - agg_df['ind_earnings_5x_contrib'] - agg_df['ind_cash_5x_contrib']\n",
        "\n",
        "for i in ['ind_sales_5x_contrib','ind_earnings_5x_contrib','ind_cash_5x_contrib','ind_price_5x_contrib']:\n",
        "    agg_df[i] = agg_df[i] + (agg_df['ind_tsr_5x']-agg_df['ind_tsr_implied_5x'])/4.0\n",
        "\n",
        "#agg_df.to_csv('industry_aggregate_test_20170804.csv', index=False)\n",
        "\n",
        "# Create a 3-year average capex-sales ratio for benchmarking purposes.\n",
        "agg_df['capex_sales'] = -1*agg_df['capex'] / agg_df['sales']\n",
        "agg_df['capex_sales_1'] = agg_df.groupby(['sector'])['capex_sales'].transform(lambda x: x.shift(-1))\n",
        "agg_df['capex_sales_2'] = agg_df.groupby(['sector'])['capex_sales'].transform(lambda x: x.shift(-2))\n",
        "agg_df['ind_capex_sales_avg'] = (agg_df['capex_sales'] + agg_df['capex_sales_1'] + agg_df['capex_sales_2']) / 3.0\n",
        "\n",
        "# Create a 3-year average R&D-sales ratio for firms that report R&D.\n",
        "rd_df = df_all[df_all['r&d']>0].groupby([\"sector\", \"start_year\"])[['sales','r&d']].sum().reset_index()\n",
        "rd_df['r&d_sales'] = rd_df['r&d'] / rd_df['sales']\n",
        "rd_df['r&d_sales_1'] = rd_df.groupby(['sector'])['r&d_sales'].transform(lambda x: x.shift(-1))\n",
        "rd_df['r&d_sales_2'] = rd_df.groupby(['sector'])['r&d_sales'].transform(lambda x: x.shift(-2))\n",
        "rd_df['ind_r&d_sales_avg'] = (rd_df['r&d_sales'] + rd_df['r&d_sales_1'] + rd_df['r&d_sales_2']) / 3.0\n",
        "\n",
        "merge_df = agg_df[['sector','start_year','ind_sales_2_contrib','ind_earnings_2_contrib','ind_cash_2_contrib','ind_price_2_contrib','ind_sales_1x_contrib','ind_earnings_1x_contrib','ind_cash_1x_contrib','ind_price_1x_contrib','ind_sales_5x_contrib','ind_earnings_5x_contrib','ind_cash_5x_contrib','ind_price_5x_contrib','ind_capex_sales_avg']].merge(rd_df[['sector','start_year','ind_r&d_sales_avg']])\n",
        "\n",
        "# Merge the industry aggregate data into the master DataFrame.\n",
        "ceo_out = df_all.merge(merge_df, how='left', on=['sector','start_year'])\n",
        "ceo_out.describe()\n",
        "\n",
        "ceo_out.to_csv('ceo_out.csv', index = False)\n",
        "\n",
        "\n",
        "\"\"\"### EXPORT TURNAROUND SAMPLE:\"\"\"\n",
        "\n",
        "ceo_sample = ceo_out\n",
        "\n",
        "# CRITERIA:\n",
        "\n",
        "# Market cap should be greater than 10B.\n",
        "#ceo_sample = ceo_sample[ceo_sample['mc'] > 10000]\n",
        "\n",
        "print(ceo_sample['start_year'].value_counts())\n",
        "\n",
        "# All TSRs should be known.\n",
        "#ceo_sample.dropna(subset=['tsr_5p','tsr_5x','ind_tsr_5p','ind_tsr_5x'], how='any', inplace=True)\n",
        "# All TSRs should be known.\n",
        "ceo_sample.dropna(subset=['tsr_2','tsr_1x','ind_tsr_2','ind_tsr_1x'], how='any', inplace=True)\n",
        "\n",
        "print(ceo_sample['start_year'].value_counts())\n",
        "\n",
        "# Only include odd years (to avoid overlapping 2-year samples) and years after 2004.\n",
        "ceo_sample = ceo_sample[ceo_sample['start_year'] % 2 == 0]\n",
        "ceo_sample = ceo_sample[ceo_sample['start_year'] > 2007]\n",
        "\n",
        "print(ceo_sample['start_year'].value_counts())\n",
        "\n",
        "# Exclude mutual funds, etc.\n",
        "#fund_list = pd.read_csv('fund_list.csv', index_col=False)[['ciq','fund']]\n",
        "#fund_list.rename(columns={'ciq': 'ciq_id'}, inplace=True)\n",
        "#ceo_sample = ceo_sample.merge(fund_list, how='left', on='ciq_id')\n",
        "\n",
        "#ceo_sample = ceo_sample[ceo_sample['fund']!='Y'].drop('fund',axis=1)\n",
        "\n",
        "#ceo_sample.describe()\n",
        "\n",
        "# Create a new column 'ceo_bool' to indicate whether the CEO data is available or not.\n",
        "ceo_sample['ceo_bool'] = np.where( ((pd.isnull(ceo_sample['ceo'])) & (pd.isnull(ceo_sample['ceo_1x']))),\"N\",\"Y\")\n",
        "ceo_sample.head()\n",
        "\n",
        "print(ceo_sample['start_year'].value_counts())\n",
        "\n",
        "# Export the sample to a CSV file for further analysis.\n",
        "ceo_sample.to_csv('ceo_output_20231127_sample.csv', index=False)\n",
        "\n",
        "df_decline = ceo_sample[ceo_sample['cat_2']=='V_neg']\n",
        "#df_decline.describe()\n",
        "\n",
        "df_decline.to_csv('ceo_output_20231127_decline.csv', index=False)\n",
        "pickle.dump(df_decline, open('ceo_output_20231127_decline.p', 'wb'))"
      ],
      "metadata": {
        "id": "h52W0egRlbJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataframe\n",
        "# df_all = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# Ensure the correct columns are of numeric type\n",
        "cols_to_convert = ['sp', 'sp_adj', 'sales', 'ebitda', 'eps', 'capex', 'mc', 'r&d', 'restructuring']\n",
        "for col in cols_to_convert:\n",
        "    df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
        "\n",
        "# Generate descriptive statistics\n",
        "df_all.describe(include='all')\n",
        "\n",
        "# Handle divisions ensuring we don't divide by zero\n",
        "df_all['shares'] = np.where(df_all['sp'] != 0, df_all['mc'] / df_all['sp'], 0)\n",
        "df_all['earnings_tot'] = df_all['eps'] * df_all['shares']\n",
        "df_all['earnings_tot'] = np.where(df_all['earnings_tot'] < 0, 0, df_all['earnings_tot'])\n",
        "df_all['mc_adj'] = df_all['sp_adj'] * df_all['shares']\n",
        "\n",
        "# Fill NaN values in relevant columns\n",
        "df_all[['shares', 'earnings_tot', 'mc_adj']] = df_all[['shares', 'earnings_tot', 'mc_adj']].fillna(value=0)\n",
        "\n",
        "# Calculate industry aggregates\n",
        "agg_df = df_all[df_all['sp'] > 0].groupby([\"sector\", \"start_year\"])[['capex', 'sales', 'earnings_tot', 'mc', 'mc_adj']].sum().reset_index()\n",
        "agg_df = agg_df.merge(df_all.groupby(['sector', 'start_year'])[['ind_tsr_5p', 'ind_tsr_2', 'ind_tsr_1x', 'ind_tsr_5x']].first().reset_index(), how='left', on=['sector', 'start_year'])\n",
        "\n",
        "# Handle divisions ensuring we don't divide by zero for capex-sales ratio\n",
        "agg_df['capex_sales'] = np.where(agg_df['sales'] != 0, -1 * agg_df['capex'] / agg_df['sales'], 0)\n",
        "\n",
        "# For firms that report R&D\n",
        "rd_df = df_all[df_all['r&d'] > 0].groupby([\"sector\", \"start_year\"])[['sales', 'r&d']].sum().reset_index()\n",
        "rd_df['r&d_sales'] = np.where(rd_df['sales'] != 0, rd_df['r&d'] / rd_df['sales'], 0)\n",
        "\n",
        "# Generate descriptive statistics\n",
        "df_all.describe(include='all')"
      ],
      "metadata": {
        "id": "RgZRakXflbxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 CEO CHANGE ANALYSIS"
      ],
      "metadata": {
        "id": "fieJsHYXloJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "import glob\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "pd.options.display.max_rows = 99\n",
        "pd.options.display.max_columns = 99\n",
        "\n",
        "helper_df = pd.read_csv('231120_company_id.csv')[['ciq_id','PERMID']]\n",
        "helper_df.rename(columns={'PERMID': 'permid'}, inplace=True)\n",
        "helper_df = helper_df[helper_df['permid']!=\"0\"]\n",
        "\n",
        "ceo_final = pickle.load(open(\"ceo_final.p\", \"rb\" ))\n",
        "ceo_final['start_year'] = pd.to_numeric(ceo_final['start_year'])\n",
        "ceo_final['end_year'] = pd.to_numeric(ceo_final['end_year'], errors='ignore')\n",
        "\n",
        "ciq_df = pickle.load(open(\"ciq_data_20231120.p\", \"rb\" ))\n",
        "ciq_df.rename(columns={'year': 'start_year', 'sp_adj':'sp_adj_0x'}, inplace=True)\n",
        "\n",
        "helper_2 = pd.read_csv('231120_company_id.csv')[['ciq_id','industry_sector']]\n",
        "helper_2.rename(columns={'industry_sector':'sector'}, inplace=True)\n",
        "\n",
        "ciq_df = ciq_df[['ciq_id','name','start_year','date','mc','sp_adj_0x']].merge(helper_2, how='left', on='ciq_id')\n",
        "\n",
        "ciq_df['sp_adj_3'] = ciq_df.groupby(['ciq_id'])['sp_adj_0x'].transform(lambda x: x.shift(3))\n",
        "ciq_df['sp_adj_2'] = ciq_df.groupby(['ciq_id'])['sp_adj_0x'].transform(lambda x: x.shift(2))\n",
        "ciq_df['sp_adj_1'] = ciq_df.groupby(['ciq_id'])['sp_adj_0x'].transform(lambda x: x.shift(1))\n",
        "ciq_df['tsr_3'] = ciq_df['sp_adj_2'] / ciq_df['sp_adj_3'] - 1\n",
        "ciq_df['tsr_2'] = ciq_df['sp_adj_1'] / ciq_df['sp_adj_2'] - 1\n",
        "ciq_df['tsr_1'] = ciq_df['sp_adj_0x'] / ciq_df['sp_adj_1'] - 1\n",
        "\n",
        "for i in range(1,6):\n",
        "    label = 'sp_adj_' + str(i) + 'x'\n",
        "    ciq_df[label] = ciq_df.groupby(['ciq_id'])['sp_adj_0x'].transform(lambda x: x.shift(-1*i))\n",
        "    label_prev = 'sp_adj_' + str(i-1) + 'x'\n",
        "    label_ts = 'tsr_'+ str(i) +'x'\n",
        "    ciq_df[label_ts] = ciq_df[label] / ciq_df[label_prev] - 1\n",
        "\n",
        "ciq_df.tail()\n",
        "\n",
        "ind_df = pd.read_csv(\"industry_data_20231127.csv\")\n",
        "ind_df['sector'] = ind_df['industry'].map(lambda x: x[8:-9])\n",
        "ind_df.loc[ind_df.sector==\"Health Care\", 'sector'] = \"Healthcare\"\n",
        "\n",
        "ind_df.drop(['industry', 'index', 'date', 'ciq total return gross', 'check industry', 'sp', 'growth'], axis=1, inplace=True)\n",
        "ind_df.rename(columns={'total return':'ind_sp_0x', 'year':'start_year', 'ciq':'ciq_id'}, inplace=True)\n",
        "ind_df = ind_df[ind_df.ind_sp_0x != 0]\n",
        "\n",
        "ind_df['ind_sp_3'] = ind_df.groupby(['sector'])['ind_sp_0x'].transform(lambda x: x.shift(3))\n",
        "ind_df['ind_sp_2'] = ind_df.groupby(['sector'])['ind_sp_0x'].transform(lambda x: x.shift(2))\n",
        "ind_df['ind_sp_1'] = ind_df.groupby(['sector'])['ind_sp_0x'].transform(lambda x: x.shift(1))\n",
        "ind_df['ind_tsr_3'] = ind_df['ind_sp_2'] / ind_df['ind_sp_3'] - 1\n",
        "ind_df['ind_tsr_2'] = ind_df['ind_sp_1'] / ind_df['ind_sp_2'] - 1\n",
        "ind_df['ind_tsr_1'] = ind_df['ind_sp_0x'] / ind_df['ind_sp_1'] - 1\n",
        "\n",
        "for i in range(1,6):\n",
        "    label = 'ind_sp_' + str(i) + 'x'\n",
        "    ind_df[label] = ind_df.groupby(['sector'])['ind_sp_0x'].transform(lambda x: x.shift(-1*i))\n",
        "    label_prev = 'ind_sp_' + str(i-1) + 'x'\n",
        "    label_ts = 'ind_tsr_'+ str(i) +'x'\n",
        "    ind_df[label_ts] = ind_df[label] / ind_df[label_prev] - 1\n",
        "\n",
        "ind_df.tail()\n",
        "\n",
        "merge_df = ciq_df.merge(ind_df[['sector','start_year','ind_tsr_3','ind_tsr_2','ind_tsr_1','ind_tsr_1x','ind_tsr_2x','ind_tsr_3x','ind_tsr_4x','ind_tsr_5x']], how='left', on=['sector','start_year'])\n",
        "merge_df.head(20)\n",
        "\n",
        "for i in ['3','2','1','1x','2x','3x','4x','5x']:\n",
        "    merge_df['tsr_delta_'+i] = merge_df['tsr_'+i] - merge_df['ind_tsr_'+i]\n",
        "\n",
        "for i in ['3','2','1','1x','2x','3x','4x','5x']:\n",
        "    merge_df['tsr_index_'+i] = (merge_df['tsr_delta_'+i] - merge_df['tsr_delta_1'])\n",
        "\n",
        "merge_df.head(20)\n",
        "\n",
        "df_decline = pickle.load(open('ceo_output_20231127_decline.p', 'rb'))\n",
        "\n",
        "#df_out = df_decline[['ciq_id','start_year','ceo_bool','ceo','prev_title','ceo_1x','ceo_title_1x','name_x','cat_2','cat_1x','cat_5x']]\n",
        "###Removing name - unclear how it was used\n",
        "df_out = df_decline[['ciq_id','start_year','ceo_bool','ceo','prev_title','ceo_1x','ceo_title_1x','cat_2','cat_1x','cat_5x']]\n",
        "df_out = df_out.merge(merge_df[['ciq_id','start_year','tsr_index_3','tsr_index_2','tsr_index_1','tsr_index_1x','tsr_index_2x','tsr_index_3x','tsr_index_4x','tsr_index_5x']], how='left', on=['ciq_id','start_year'])\n",
        "\n",
        "df_out.describe(include='all')\n",
        "\n",
        "df_out.to_csv('ceo_change_analysis.csv', index=False)"
      ],
      "metadata": {
        "id": "dIiKtAUcozfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. INVESTMENT ANALYSIS"
      ],
      "metadata": {
        "id": "5ojRWMZkldTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "import glob\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import scipy.stats\n",
        "\n",
        "# Setting the display options for pandas DataFrame to view more rows and columns\n",
        "pd.options.display.max_rows = 99\n",
        "pd.options.display.max_columns = 99\n",
        "\n",
        "# Loading a DataFrame from CSV to map company ids from one format to another\n",
        "helper_df = pd.read_csv('231120_company_id.csv')[['ciq_id','PERMID']]\n",
        "helper_df.rename(columns={'PERMID': 'permid'}, inplace=True)  # Renaming column for consistency\n",
        "helper_df = helper_df[helper_df['permid']!=\"0\"]  # Filtering out rows where permid is \"0\"\n",
        "\n",
        "# Loading CEO data from a pickle file and converting years to numeric\n",
        "ceo_final = pickle.load(open(\"ceo_final_v2.p\", \"rb\" ))\n",
        "ceo_final['start_year'] = pd.to_numeric(ceo_final['start_year'])\n",
        "ceo_final['end_year'] = pd.to_numeric(ceo_final['end_year'], errors='ignore')  # Ignoring errors in conversion\n",
        "\n",
        "# Loading another DataFrame from a pickle file and renaming columns for consistency\n",
        "ciq_load = pickle.load(open(\"ciq_data_20231120.p\", \"rb\" ))\n",
        "ciq_load.rename(columns={'year': 'start_year', 'sp_adj':'sp_adj_0x'}, inplace=True)\n",
        "\n",
        "# Loading another DataFrame to get industry sector information\n",
        "helper_2 = pd.read_csv('231120_company_id.csv')[['ciq_id','industry_sector']]\n",
        "helper_2.rename(columns={'industry_sector':'sector'}, inplace=True)\n",
        "\n",
        "# --- Calculating Capex to Sales Ratio ---\n",
        "ciq_df = ciq_load[['ciq_id','name','start_year','date','capex','r&d','sales']].merge(helper_2, how='left', on='ciq_id')  # Merging data on common 'ciq_id' to get a consolidated DataFrame\n",
        "ciq_df = ciq_df[ciq_df['sales']>0]  # Filtering out rows where sales are zero to avoid division by zero\n",
        "ciq_df['capex'] = ciq_df['capex']*-1  # Multiplying Capex by -1, possibly to correct sign\n",
        "ciq_df['capex_sales'] = ciq_df['capex']/ciq_df['sales']  # Calculating Capex to Sales ratio\n",
        "ciq_df.head()\n",
        "\n",
        "# Aggregating Capex and Sales at the sector and start_year level to calculate industry level Capex to Sales ratio\n",
        "ind_df = ciq_df.groupby(['sector','start_year'])['capex','sales'].sum().reset_index()\n",
        "ind_df['ind_capex_sales'] = ind_df['capex']/ind_df['sales']  # Calculating industry level Capex to Sales ratio\n",
        "ind_df.head()\n",
        "\n",
        "# Merging the firm level and industry level Capex to Sales ratio data\n",
        "capex_df = ciq_df[['ciq_id','name','start_year','sector','capex_sales']].merge(ind_df[['sector','start_year','ind_capex_sales']], how='left', on=['sector','start_year'])\n",
        "capex_df['capex_sales_delta'] = capex_df['capex_sales'] - capex_df['ind_capex_sales']  # Calculating the difference between firm level and industry level Capex to Sales ratio\n",
        "capex_df.capex_sales_delta = scipy.stats.mstats.winsorize(capex_df.capex_sales_delta, limits=0.04)  # Winsorizing the data to mitigate the effect of outliers\n",
        "\n",
        "# Calculating the change in Capex to Sales ratio over different periods\n",
        "for i in range(1,6):\n",
        "    capex_df['capex_sales_delta_'+str(i)+'x'] = capex_df.groupby(['ciq_id'])['capex_sales_delta'].transform(lambda x: x.shift(-1*i))\n",
        "\n",
        "# Calculating the average and standard deviation of Capex to Sales ratio change\n",
        "capex_df['capex_sales_delta_avg'] = (capex_df['capex_sales_delta'] + capex_df['capex_sales_delta_1x'] + capex_df['capex_sales_delta_2x'])/3.0\n",
        "capex_df['capex_sales_delta_std'] = capex_df.groupby(['start_year'])['capex_sales_delta_avg'].transform(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "capex_df.describe()\n",
        "\n",
        "# --- Calculating Firm R&D to Sales Ratio ---\n",
        "ciq_df = ciq_load[['ciq_id','name','start_year','date','capex','r&d','sales']].merge(helper_2, how='left', on='ciq_id')  # Repeating the merge to ensure data consistency\n",
        "ciq_df = ciq_df[ciq_df['sales']>0]  # Filtering out rows where sales are zero to avoid division by zero\n",
        "ciq_df = ciq_df[ciq_df['r&d']>0]  # Filtering out rows where R&D is zero\n",
        "ciq_df['rd_sales'] = ciq_df['r&d']/ciq_df['sales']  # Calculating R&D to Sales ratio\n",
        "ciq_df.head()\n",
        "\n",
        "# Aggregating R&D and Sales at the sector and start_year level to calculate industry level R&D to Sales ratio\n",
        "ind_df = ciq_df.groupby(['sector','start_year'])['r&d','sales'].sum().reset_index()\n",
        "ind_df['ind_rd_sales'] = ind_df['r&d']/ind_df['sales']  # Calculating industry level R&D to Sales ratio\n",
        "ind_df.head()\n",
        "\n",
        "# Merging the firm level and industry level R&D to Sales ratio data\n",
        "rd_df = ciq_df[['ciq_id','name','start_year','sector','rd_sales']].merge(ind_df[['sector','start_year','ind_rd_sales']], how='left', on=['sector','start_year'])\n",
        "rd_df['rd_sales_delta'] = rd_df['rd_sales'] - rd_df['ind_rd_sales']  # Calculating the difference between firm level and industry level R&D to Sales ratio\n",
        "rd_df.rd_sales_delta = scipy.stats.mstats.winsorize(rd_df.rd_sales_delta, limits=0.04)  # Winsorizing the data to mitigate the effect of outliers\n",
        "\n",
        "# Calculating the change in R&D to Sales ratio over different periods\n",
        "for i in range(1,6):\n",
        "    rd_df['rd_sales_delta_'+str(i)+'x'] = rd_df.groupby(['ciq_id'])['rd_sales_delta'].transform(lambda x: x.shift(-1*i))\n",
        "\n",
        "# Calculating the average of R&D to Sales ratio change\n",
        "rd_df['rd_sales_delta_avg'] = (rd_df['rd_sales_delta'] + rd_df['rd_sales_delta_1x'] + rd_df['rd_sales_delta_2x'])/3.0\n",
        "\n",
        "# Calculating the standard deviation of R&D to Sales ratio change\n",
        "rd_df['rd_sales_delta_std'] = rd_df.groupby(['start_year'])['rd_sales_delta_avg'].transform(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "rd_df.describe()\n",
        "\n",
        "# --- Merging into Downturn Database ---\n",
        "df_decline = pickle.load(open('ceo_output_20231127_decline.p', 'rb'))\n",
        "df_decline['exclude'] = np.where(df_decline['sp_adj_5x']==0, \"Y\",\"N\")  # Creating an 'exclude' flag based on a condition\n",
        "\n",
        "# Preparing the output DataFrame by selecting specific columns and merging the investment analysis data\n",
        "df_out = df_decline[['ciq_id','start_year','ceo_bool','ceo','prev_title','ceo_1x','sector','exclude','cat_2','cat_1x','cat_5x','tsr_2_chg','tsr_1x_chg','tsr_5x_chg','net_longterm_avg']]\n",
        "#df_out = df_decline[['ciq_id','start_year','ceo_bool','ceo','prev_title','ceo_1x','ceo_title_1x','name_x','sector','exclude','cat_2','cat_1x','cat_5x','tsr_2_chg','tsr_1x_chg','tsr_5x_chg','net_longterm_avg','net_bio_avg']]\n",
        "df_out = df_out.merge(capex_df.drop(['name','sector'], axis=1), how='left', on=['ciq_id','start_year']).merge(rd_df.drop(['name','sector'], axis=1), how='left', on=['ciq_id','start_year'])\n",
        "\n",
        "# Getting descriptive statistics of the final DataFrame\n",
        "df_out.describe(include='all')\n",
        "\n",
        "# Saving the final DataFrame to a CSV file\n",
        "df_out.to_csv('investment_analysis_20231122.csv', index=False)"
      ],
      "metadata": {
        "id": "UcHUHavbo9nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. TSR PATHS"
      ],
      "metadata": {
        "id": "ZeC-YoVvlnpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###TAKES: ceo_final_v2.p, 231120_copmany_id.csv, ciq_data.p, ceo_final, industry_data\n",
        "\n",
        "import pandas as pd  # Importing the pandas library for data manipulation and analysis\n",
        "import numpy as np  # Importing the numpy library for numerical operations\n",
        "import time  # Importing the time module to handle time-related tasks\n",
        "import pickle  # Importing the pickle module for serializing and deserializing Python object structures\n",
        "\n",
        "import glob  # Importing the glob module to find all the pathnames matching a specified pattern\n",
        "import json  # Importing the json module to work with JSON data\n",
        "from bs4 import BeautifulSoup  # Importing BeautifulSoup from bs4 module for web scraping tasks\n",
        "\n",
        "# Setting display options for pandas DataFrame to display 99 rows and columns at max\n",
        "pd.options.display.max_rows = 99\n",
        "pd.options.display.max_columns = 99\n",
        "\n",
        "# Reading a CSV file to get company identifiers and renaming the 'PERMID' column to 'permid'\n",
        "helper_df = pd.read_csv('231120_company_id.csv')[['ciq_id','PERMID']]\n",
        "helper_df.rename(columns={'PERMID': 'permid'}, inplace=True)\n",
        "# Filtering out the rows where 'permid' is not \"0\"\n",
        "helper_df = helper_df[helper_df['permid']!=\"0\"]\n",
        "\n",
        "# Loading a pickled (serialized) object into a pandas DataFrame, which contains final data regarding CEOs\n",
        "ceo_final = pickle.load(open(\"ceo_final_v2.p\", \"rb\" ))\n",
        "# Converting the 'start_year' and 'end_year' columns to numeric type\n",
        "ceo_final['start_year'] = pd.to_numeric(ceo_final['start_year'])\n",
        "ceo_final['end_year'] = pd.to_numeric(ceo_final['end_year'], errors='ignore')  # Ignoring any errors during conversion\n",
        "\n",
        "# Loading another pickled object containing CIQ data into a pandas DataFrame and selecting specific columns\n",
        "ciq_df = pickle.load(open(\"ciq_data_20231120.p\", \"rb\" ))[['ciq_id','name','start_year','sp_adj','sales','ebitda','eps','sp','mc']]\n",
        "# Renaming the 'year' column to 'start_year'\n",
        "#ciq_df.rename(columns={'year': 'start_year'}, inplace=True)\n",
        "\n",
        "# Reading another CSV file to get the industry sector information for companies\n",
        "helper_2 = pd.read_csv('231120_company_id.csv')[['ciq_id','industry_sector']]\n",
        "# Renaming the 'industry_sector' column to 'sector'\n",
        "helper_2.rename(columns={'industry_sector':'sector'}, inplace=True)\n",
        "\n",
        "# Merging the 'ciq_df' DataFrame with 'helper_2' DataFrame on 'ciq_id' to get the sector information for each company\n",
        "ciq_df = ciq_df.merge(helper_2, how='left', on='ciq_id')\n",
        "\n",
        "# Displaying the first 20 rows of the 'ciq_df' DataFrame\n",
        "ciq_df.head(20)\n",
        "\n",
        "# This part of code is dedicated to calculating Firm TSR (Total Shareholder Return) Contribution\n",
        "\"\"\"### FIRM TSR CONTRIBUTION\"\"\"\n",
        "\n",
        "# List of financial metrics for which the growth and contribution will be calculated\n",
        "var_list = ['sp_adj','sales','ebitda','eps','sp','mc']\n",
        "'''\n",
        "# Renaming the columns to indicate the base year values (0x) and calculating the values for subsequent years (1x to 5x) along with growth rates\n",
        "for k in var_list:\n",
        "    ciq_df.rename(columns={str(k): str(k)+\"_0x\"}, inplace=True)  # Renaming the columns to indicate the base year values (0x)\n",
        "    for i in range(1,6):\n",
        "        label = str(k) + '_' + str(i) + 'x'  # Creating the label for subsequent years (1x to 5x)\n",
        "        base_label = str(k)+'_0x'  # Base label indicating the base year values\n",
        "        # Shifting the values for each year and calculating the growth rates\n",
        "        ciq_df[label] = ciq_df.groupby(['ciq_id'])[base_label].transform(lambda x: x.shift(-1*i))\n",
        "        growth_label = label + '_growth'\n",
        "        ciq_df[growth_label] = np.power((ciq_df[label]/ciq_df[base_label]), (1.0/(i)))-1\n",
        "'''\n",
        "for k in var_list:\n",
        "    ciq_df.rename(columns={str(k): str(k)+\"_0x\"}, inplace=True)  # Renaming the columns to indicate the base year values (0x)\n",
        "    for i in range(1,6):\n",
        "        label = str(k) + '_' + str(i) + 'x'  # Creating the label for subsequent years (1x to 5x)\n",
        "        base_label = str(k)+'_0x'  # Base label indicating the base year values\n",
        "        # Shifting the values for each year and calculating the growth rates\n",
        "        ciq_df[label] = ciq_df.groupby(['ciq_id'])[base_label].transform(lambda x: x.shift(-1*i))\n",
        "        ciq_df[label] = pd.to_numeric(ciq_df[label], errors='coerce')\n",
        "        ciq_df[base_label] = pd.to_numeric(ciq_df[base_label], errors='coerce')\n",
        "        growth_label = label + '_growth'\n",
        "        ciq_df[growth_label] = np.power((ciq_df[label]/ciq_df[base_label]), (1.0/(i)))-1\n",
        "\n",
        "\n",
        "# Displaying the last few rows of the 'ciq_df' DataFrame\n",
        "ciq_df.tail()\n",
        "\n",
        "# This section calculates the contributions of sales, earnings, cash, and price to TSR for 1x, 2x, 3x, 4x, and 5x (1 year, 2 years, ... up to 5 years)\n",
        "\n",
        "# CALCULATE CONTRIBUTIONS FOR 1 YEAR (1x)\n",
        "ciq_df['sales_1x_contrib'] = ciq_df['sales_1x_growth']  # Sales contribution is simply the growth rate of sales\n",
        "ciq_df['earnings_1x_contrib'] = ciq_df['ebitda_1x_growth'] - ciq_df['sales_1x_contrib']  # Earnings contribution is the growth rate of EBITDA minus sales contribution\n",
        "ciq_df['cash_1x_contrib'] = ciq_df['sp_adj_1x_growth'] - ciq_df['sp_1x_growth']  # Cash contribution is the growth rate of adjusted share price minus share price growth\n",
        "\n",
        "# Several corrections are made to ensure the accuracy and reasonableness of the calculated contributions\n",
        "# If EPS data is missing, EBITDA growth is used instead for earnings contribution\n",
        "ciq_df['earnings_1x_contrib'] = np.where( (np.isnan(ciq_df['earnings_1x_contrib'])), ciq_df['eps_1x_growth'] - ciq_df['sales_1x_contrib'], ciq_df['earnings_1x_contrib'])\n",
        "# If earnings switch from negative to positive, the total TSR growth is used as the earnings contribution\n",
        "ciq_df['earnings_1x_contrib'] = np.where( ((ciq_df['ebitda_0x'] < 0) & (ciq_df['ebitda_1x'] > 0)), ciq_df['sp_adj_1x_growth'], ciq_df['earnings_1x_contrib'])\n",
        "ciq_df['earnings_1x_contrib'] = np.where( ((ciq_df['ebitda_0x'] > 0) & (ciq_df['ebitda_1x'] < 0)), ciq_df['sp_adj_1x_growth'], ciq_df['earnings_1x_contrib'])\n",
        "# If earnings are still negative, the growth rate is inverted\n",
        "ciq_df['earnings_1x_contrib'] = np.where( (ciq_df['ebitda_1x'] < 0), -1*ciq_df['earnings_1x_contrib'], ciq_df['earnings_1x_contrib'])\n",
        "# The earnings contribution is capped at 100% to prevent unreasonable values (e.g., 88x growth)\n",
        "ciq_df['earnings_1x_contrib'] = np.where( (ciq_df['earnings_1x_contrib'] < -1), -1, ciq_df['earnings_1x_contrib'])\n",
        "ciq_df['earnings_1x_contrib'] = np.where( (ciq_df['earnings_1x_contrib'] > 1), 1, ciq_df['earnings_1x_contrib'])\n",
        "\n",
        "# Calculating the price contribution as the residual of TSR growth after accounting for sales, earnings, and cash contributions\n",
        "ciq_df['price_1x_contrib'] = ciq_df['sp_adj_1x_growth'] - ciq_df['sales_1x_contrib'] - ciq_df['earnings_1x_contrib'] - ciq_df['cash_1x_contrib']\n",
        "\n",
        "# Similar calculations are repeated for 2 years (2x), 3 years (3x), 4 years (4x), and 5 years (5x)\n",
        "\n",
        "# CALCULATE CONTRIBUTIONS FOR 2 YEARS (2x)\n",
        "ciq_df['sales_2x_contrib'] = ciq_df['sales_2x_growth']\n",
        "ciq_df['earnings_2x_contrib'] = ciq_df['ebitda_2x_growth'] - ciq_df['sales_2x_contrib']\n",
        "ciq_df['cash_2x_contrib'] = ciq_df['sp_adj_2x_growth'] - ciq_df['sp_2x_growth']\n",
        "# Corrections\n",
        "ciq_df['earnings_2x_contrib'] = np.where( (np.isnan(ciq_df['earnings_2x_contrib'])), ciq_df['eps_2x_growth'] - ciq_df['sales_2x_contrib'], ciq_df['earnings_2x_contrib'])\n",
        "ciq_df['earnings_2x_contrib'] = np.where( ((ciq_df['ebitda_0x'] < 0) & (ciq_df['ebitda_2x'] > 0)), ciq_df['sp_adj_2x_growth'], ciq_df['earnings_2x_contrib'])\n",
        "ciq_df['earnings_2x_contrib'] = np.where( ((ciq_df['ebitda_0x'] > 0) & (ciq_df['ebitda_2x'] < 0)), ciq_df['sp_adj_2x_growth'], ciq_df['earnings_2x_contrib'])\n",
        "ciq_df['earnings_2x_contrib'] = np.where( (ciq_df['ebitda_2x'] < 0), -1*ciq_df['earnings_2x_contrib'], ciq_df['earnings_2x_contrib'])\n",
        "ciq_df['earnings_2x_contrib'] = np.where( (ciq_df['earnings_2x_contrib'] < -1), -1, ciq_df['earnings_2x_contrib'])\n",
        "ciq_df['earnings_2x_contrib'] = np.where( (ciq_df['earnings_2x_contrib'] > 1), 1, ciq_df['earnings_2x_contrib'])\n",
        "ciq_df['price_2x_contrib'] = ciq_df['sp_adj_2x_growth'] - ciq_df['sales_2x_contrib'] - ciq_df['earnings_2x_contrib'] - ciq_df['cash_2x_contrib']\n",
        "\n",
        "# CALCULATE CONTRIBUTIONS FOR 3 YEARS (3x)\n",
        "ciq_df['sales_3x_contrib'] = ciq_df['sales_3x_growth']\n",
        "ciq_df['earnings_3x_contrib'] = ciq_df['ebitda_3x_growth'] - ciq_df['sales_3x_contrib']\n",
        "ciq_df['cash_3x_contrib'] = ciq_df['sp_adj_3x_growth'] - ciq_df['sp_3x_growth']\n",
        "# Corrections\n",
        "ciq_df['earnings_3x_contrib'] = np.where( (np.isnan(ciq_df['earnings_3x_contrib'])), ciq_df['eps_3x_growth'] - ciq_df['sales_3x_contrib'], ciq_df['earnings_3x_contrib'])\n",
        "ciq_df['earnings_3x_contrib'] = np.where( ((ciq_df['ebitda_0x'] < 0) & (ciq_df['ebitda_3x'] > 0)), ciq_df['sp_adj_3x_growth'], ciq_df['earnings_3x_contrib'])\n",
        "ciq_df['earnings_3x_contrib'] = np.where( ((ciq_df['ebitda_0x'] > 0) & (ciq_df['ebitda_3x'] < 0)), ciq_df['sp_adj_3x_growth'], ciq_df['earnings_3x_contrib'])\n",
        "ciq_df['earnings_3x_contrib'] = np.where( (ciq_df['ebitda_3x'] < 0), -1*ciq_df['earnings_3x_contrib'], ciq_df['earnings_3x_contrib'])\n",
        "ciq_df['earnings_3x_contrib'] = np.where( (ciq_df['earnings_3x_contrib'] < -1), -1, ciq_df['earnings_3x_contrib'])\n",
        "ciq_df['earnings_3x_contrib'] = np.where( (ciq_df['earnings_3x_contrib'] > 1), 1, ciq_df['earnings_3x_contrib'])\n",
        "ciq_df['price_3x_contrib'] = ciq_df['sp_adj_3x_growth'] - ciq_df['sales_3x_contrib'] - ciq_df['earnings_3x_contrib'] - ciq_df['cash_3x_contrib']\n",
        "\n",
        "# * 4x *\n",
        "\n",
        "# Calculating sales contribution for the 4th period by simply assigning the growth value\n",
        "ciq_df['sales_4x_contrib'] = ciq_df['sales_4x_growth']\n",
        "\n",
        "# Calculating earnings contribution for the 4th period by subtracting sales contribution from ebitda growth\n",
        "ciq_df['earnings_4x_contrib'] = ciq_df['ebitda_4x_growth'] - ciq_df['sales_4x_contrib']\n",
        "\n",
        "# Calculating cash contribution for the 4th period by subtracting the share price growth from adjusted share price growth\n",
        "ciq_df['cash_4x_contrib'] = ciq_df['sp_adj_4x_growth'] - ciq_df['sp_4x_growth']\n",
        "\n",
        "# corrections:\n",
        "# if earnings is NaN, use ebitda\n",
        "# Updating earnings contribution for the 4th period based on the presence of earnings data\n",
        "ciq_df['earnings_4x_contrib'] = np.where((np.isnan(ciq_df['earnings_4x_contrib'])), ciq_df['eps_4x_growth'] - ciq_df['sales_4x_contrib'], ciq_df['earnings_4x_contrib'])\n",
        "\n",
        "# if earnings go from negative to positive, set contribution = total TSR growth:\n",
        "# Updating earnings contribution for the 4th period based on the transition of earnings from negative to positive\n",
        "ciq_df['earnings_4x_contrib'] = np.where(((ciq_df['ebitda_0x'] < 0) & (ciq_df['ebitda_4x'] > 0)), ciq_df['sp_adj_4x_growth'], ciq_df['earnings_4x_contrib'])\n",
        "ciq_df['earnings_4x_contrib'] = np.where(((ciq_df['ebitda_0x'] > 0) & (ciq_df['ebitda_4x'] < 0)), ciq_df['sp_adj_4x_growth'], ciq_df['earnings_4x_contrib'])\n",
        "\n",
        "# if earnings are still negative, invert growth rate:\n",
        "# Inverting the growth rate for the 4th period in case earnings are still negative\n",
        "ciq_df['earnings_4x_contrib'] = np.where((ciq_df['ebitda_4x'] < 0), -1 * ciq_df['earnings_4x_contrib'], ciq_df['earnings_4x_contrib'])\n",
        "\n",
        "# cap earnings contribution at 100% - arbitrary but need to cut off the 88x growth contribution\n",
        "# Capping earnings contribution for the 4th period at 100% to handle extreme growth contribution\n",
        "ciq_df['earnings_4x_contrib'] = np.where((ciq_df['earnings_4x_contrib'] < -1), -1, ciq_df['earnings_4x_contrib'])\n",
        "ciq_df['earnings_4x_contrib'] = np.where((ciq_df['earnings_4x_contrib'] > 1), 1, ciq_df['earnings_4x_contrib'])\n",
        "\n",
        "# Calculating price contribution for the 4th period by subtracting other contributions from the adjusted share price growth\n",
        "ciq_df['price_4x_contrib'] = ciq_df['sp_adj_4x_growth'] - ciq_df['sales_4x_contrib'] - ciq_df['earnings_4x_contrib'] - ciq_df['cash_4x_contrib']\n",
        "\n",
        "\n",
        "# * 5x *\n",
        "\n",
        "# The following block of code for the 5th period is similar to the 4th period,\n",
        "# with calculations done for sales, earnings, cash, and price contributions for the 5th period.\n",
        "# The logic for corrections remains the same, tailored for the 5th period data.\n",
        "\n",
        "ciq_df['sales_5x_contrib'] = ciq_df['sales_5x_growth']\n",
        "ciq_df['earnings_5x_contrib'] = ciq_df['ebitda_5x_growth'] - ciq_df['sales_5x_contrib']\n",
        "ciq_df['cash_5x_contrib'] = ciq_df['sp_adj_5x_growth'] - ciq_df['sp_5x_growth']\n",
        "\n",
        "# corrections:\n",
        "ciq_df['earnings_5x_contrib'] = np.where((np.isnan(ciq_df['earnings_5x_contrib'])), ciq_df['eps_5x_growth'] - ciq_df['sales_5x_contrib'], ciq_df['earnings_5x_contrib'])\n",
        "ciq_df['earnings_5x_contrib'] = np.where(((ciq_df['ebitda_0x'] < 0) & (ciq_df['ebitda_5x'] > 0)), ciq_df['sp_adj_5x_growth'], ciq_df['earnings_5x_contrib'])\n",
        "ciq_df['earnings_5x_contrib'] = np.where(((ciq_df['ebitda_0x'] > 0) & (ciq_df['ebitda_5x'] < 0)), ciq_df['sp_adj_5x_growth'], ciq_df['earnings_5x_contrib'])\n",
        "ciq_df['earnings_5x_contrib'] = np.where((ciq_df['ebitda_5x'] < 0), -1 * ciq_df['earnings_5x_contrib'], ciq_df['earnings_5x_contrib'])\n",
        "ciq_df['earnings_5x_contrib'] = np.where((ciq_df['earnings_5x_contrib'] < -1), -1, ciq_df['earnings_5x_contrib'])\n",
        "ciq_df['earnings_5x_contrib'] = np.where((ciq_df['earnings_5x_contrib'] > 1), 1, ciq_df['earnings_5x_contrib'])\n",
        "\n",
        "ciq_df['price_5x_contrib'] = ciq_df['sp_adj_5x_growth'] - ciq_df['sales_5x_contrib'] - ciq_df['earnings_5x_contrib'] - ciq_df['cash_5x_contrib']\n",
        "\n",
        "# The commented code line below is likely used for debugging or verification, to output the current state of the DataFrame to a CSV file.\n",
        "#ciq_df.to_csv('tsr_testing.csv')\n",
        "\n",
        "\"\"\"### INDUSTRY TSR:\"\"\"\n",
        "\n",
        "# Loading industry data from a CSV file\n",
        "ind_df = pd.read_csv(\"industry_data_20231127.csv\")\n",
        "\n",
        "# Extracting sector information from the industry column by removing the first 8 and last 9 characters from each entry\n",
        "ind_df['sector'] = ind_df['industry'].map(lambda x: x[8:-9])\n",
        "\n",
        "# Correcting sector name for \"Health Care\" to \"Healthcare\"\n",
        "ind_df.loc[ind_df.sector==\"Health Care\", 'sector'] = \"Healthcare\"\n",
        "\n",
        "# Dropping unnecessary columns to retain only the essential information for the analysis\n",
        "ind_df.drop(['industry', 'index', 'date', 'ciq total return gross', 'check industry', 'sp', 'growth'], axis=1, inplace=True)\n",
        "\n",
        "# Renaming columns for better clarity and consistency\n",
        "ind_df.rename(columns={'total return':'ind_sp_0x', 'year':'start_year', 'ciq':'ciq_id'}, inplace=True)\n",
        "\n",
        "# Filtering out entries with zero total share return to avoid distortions in the analysis\n",
        "ind_df = ind_df[ind_df.ind_sp_0x != 0]\n",
        "\n",
        "# The following block of code calculates the industry total share return (TSR) for different periods (1 to 5 years).\n",
        "# It shifts the initial total share return values for each sector to calculate the TSR for subsequent periods.\n",
        "# The TSR growth rate for each period is also calculated.\n",
        "\n",
        "for i in range(1,6):\n",
        "    label = 'ind_sp_' + str(i) + 'x'  # Label for the total share return value for the current period\n",
        "    ind_df[label] = ind_df.groupby(['sector'])['ind_sp_0x'].transform(lambda x: x.shift(-1*i))  # Shifting the initial TSR values\n",
        "    label_ts = 'ind_tsr_'+ str(i) +'x'  # Label for the TSR growth rate for the current period\n",
        "    ind_df[label_ts] = np.power((ind_df[label]/ind_df['ind_sp_0x']), (1.0/(i)))-1  # Calculating the TSR growth rate\n",
        "\n",
        "# Displaying the last 10 rows of the DataFrame for verification\n",
        "ind_df.tail(10)\n",
        "\n",
        "# Loading a DataFrame from a pickled file, which possibly contains the aggregate data for all firms\n",
        "df_all = pickle.load(open(\"ceo_final.p\", \"rb\"))\n",
        "\n",
        "# The following block of code calculates the total earnings and cash-adjusted market cap for each firm.\n",
        "# Shares are calculated by dividing the market cap by the share price.\n",
        "# Total earnings are calculated by multiplying earnings per share (EPS) by the number of shares.\n",
        "# Earnings are set to zero if they are negative (as negative earnings are not considered in the analysis).\n",
        "# Cash-adjusted market cap is calculated by multiplying the adjusted share price by the number of shares.\n",
        "\n",
        "# Ensure the correct columns are of numeric type\n",
        "cols_to_convert = ['sp', 'sp_adj', 'sales', 'ebitda', 'eps', 'capex', 'mc', 'r&d', 'restructuring']\n",
        "for col in cols_to_convert:\n",
        "    df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
        "\n",
        "# calculate total earnings, cash-adjusted market cap:\n",
        "df_all['shares'] = df_all['mc'] / df_all['sp']  # Calculating the number of shares\n",
        "df_all['earnings_tot'] = df_all['eps'] * df_all['shares']  # Calculating total earnings\n",
        "df_all['earnings_tot'] = np.where(df_all['earnings_tot'] < 0, 0, df_all['earnings_tot'])  # Setting negative earnings to zero\n",
        "df_all['mc_adj'] = df_all['sp_adj'] * df_all['shares']  # Calculating cash-adjusted market cap\n",
        "\n",
        "# Filling NaN values with zeros for the specified columns\n",
        "df_all[['shares','earnings_tot','mc_adj']] = df_all[['shares','earnings_tot','mc_adj']].fillna(value=0)\n",
        "\n",
        "# The commented code line below is likely used for debugging or verification, to view specific columns of the DataFrame.\n",
        "#df_all[['sp','sp_adj','eps','mc','mc_adj','earnings_tot','shares']].head(20)\n",
        "\n",
        "# The following block of code calculates industry aggregates for capital expenditure (capex), sales, total earnings, market cap, and cash-adjusted market cap.\n",
        "# The data is grouped by sector and start year, and the sum of the specified columns is calculated for each group.\n",
        "\n",
        "# calculate industry aggregates:\n",
        "# Grouping data by sector and start year, then summing up the values of specified columns for each group\n",
        "agg_df = df_all[df_all['sp']>0].groupby([\"sector\", \"start_year\"])[['capex','sales','earnings_tot','mc','mc_adj']].sum().reset_index()\n",
        "\n",
        "# Displaying the first 10 rows of the aggregated data for verification\n",
        "agg_df.head(10)\n",
        "\n",
        "# Merging the aggregated data with industry total share return (TSR) data based on sector and start year\n",
        "agg_df = agg_df.merge(ind_df[['sector','start_year','ind_tsr_1x','ind_tsr_2x','ind_tsr_3x','ind_tsr_4x','ind_tsr_5x']], how='left', on=['sector','start_year'])\n",
        "agg_df.head()\n",
        "\n",
        "# Renaming columns for better clarity and consistency\n",
        "agg_df.rename(columns={'sales':'ind_sales', 'earnings_tot':'ind_earnings','mc':'ind_mc','mc_adj':'ind_mc_adj'}, inplace=True)\n",
        "\n",
        "# List of financial metrics to be analyzed\n",
        "var_list = ['ind_mc','ind_mc_adj','ind_sales','ind_earnings']\n",
        "\n",
        "# Iterating through each financial metric in the list\n",
        "for k in var_list:\n",
        "    # Renaming the columns to append \"_0x\" at the end, indicating the initial value of the metric\n",
        "    agg_df.rename(columns={str(k): str(k)+\"_0x\"}, inplace=True)\n",
        "    # Iterating through a range of 1 to 6 to calculate the value and growth rate of each metric for different periods (1 to 5 years)\n",
        "    for i in range(1,6):\n",
        "        # Constructing the column label for the value of the metric for the current period\n",
        "        label = str(k) + '_' + str(i) + 'x'\n",
        "        # Constructing the column label for the initial value of the metric\n",
        "        base_label = str(k)+'_0x'\n",
        "        # Shifting the initial values of the metric to calculate the value of the metric for the current period\n",
        "        agg_df[label] = agg_df.groupby(['sector'])[base_label].transform(lambda x: x.shift(-1*i))\n",
        "        # Constructing the column label for the growth rate of the metric for the current period\n",
        "        growth_label = label + '_growth'\n",
        "        # Calculating the growth rate of the metric for the current period\n",
        "        agg_df[growth_label] = np.power((agg_df[label]/agg_df[base_label]), (1.0/(i)))-1\n",
        "# The commented code line below is likely used for debugging or verification, to view the last 10 rows of the DataFrame.\n",
        "#agg_df.tail(10)\n",
        "\n",
        "# The following block of code calculates the contributions of sales, earnings, cash, and price for the 1st period.\n",
        "# It assigns the growth rates directly for sales and calculates the contributions of earnings, cash, and price based on other financial metrics.\n",
        "# Corrections are made to the earnings contribution to handle specific scenarios, such as capping the earnings contribution at 50%.\n",
        "\n",
        "agg_df['ind_sales_1x_contrib'] = agg_df['ind_sales_1x_growth']\n",
        "agg_df['ind_earnings_1x_contrib'] = agg_df['ind_earnings_1x_growth'] - agg_df['ind_sales_1x_contrib']\n",
        "agg_df['ind_earnings_1x_contrib'] = np.where(agg_df['ind_earnings_1x_contrib']>0.5, 0.5, agg_df['ind_earnings_1x_contrib'])\n",
        "agg_df['ind_earnings_1x_contrib'] = np.where(agg_df['ind_earnings_1x_contrib']<-0.5, -0.5, agg_df['ind_earnings_1x_contrib'])  # cap earnings contribution at 50% - arbitrary but need to cut off the 6x growth contribution\n",
        "agg_df['ind_cash_1x_contrib'] = agg_df['ind_mc_adj_1x_growth'] - agg_df['ind_mc_1x_growth']\n",
        "agg_df['ind_price_1x_contrib'] = agg_df['ind_mc_adj_1x_growth'] - agg_df['ind_sales_1x_contrib'] - agg_df['ind_earnings_1x_contrib'] - agg_df['ind_cash_1x_contrib']\n",
        "\n",
        "# Distributing the difference between the industry TSR and cash-adjusted market cap growth equally among the four contributions for the 1st period\n",
        "for i in ['ind_sales_1x_contrib','ind_earnings_1x_contrib','ind_cash_1x_contrib','ind_price_1x_contrib']:\n",
        "    agg_df[i] = agg_df[i] + (agg_df['ind_tsr_1x']-agg_df['ind_mc_adj_1x_growth'])/4.0\n",
        "\n",
        "# The following blocks of code for the 2nd to 5th periods are similar to the block of code for the 1st period,\n",
        "# with calculations done for sales, earnings, cash, and price contributions for each period.\n",
        "# The logic for corrections and distributing the difference remains the same, tailored for each period's data.\n",
        "\n",
        "agg_df['ind_sales_2x_contrib'] = agg_df['ind_sales_2x_growth']\n",
        "agg_df['ind_earnings_2x_contrib'] = agg_df['ind_earnings_2x_growth'] - agg_df['ind_sales_2x_contrib']\n",
        "agg_df['ind_earnings_2x_contrib'] = np.where(agg_df['ind_earnings_2x_contrib']>0.5, 0.5, agg_df['ind_earnings_2x_contrib'])\n",
        "agg_df['ind_earnings_2x_contrib'] = np.where(agg_df['ind_earnings_2x_contrib']<-0.5, -0.5, agg_df['ind_earnings_2x_contrib'])  # cap earnings contribution at 50% - arbitrary but need to cut off the 6x growth contribution\n",
        "agg_df['ind_cash_2x_contrib'] = agg_df['ind_mc_adj_2x_growth'] - agg_df['ind_mc_2x_growth']\n",
        "agg_df['ind_price_2x_contrib'] = agg_df['ind_mc_adj_2x_growth'] - agg_df['ind_sales_2x_contrib'] - agg_df['ind_earnings_2x_contrib'] - agg_df['ind_cash_2x_contrib']\n",
        "\n",
        "for i in ['ind_sales_2x_contrib','ind_earnings_2x_contrib','ind_cash_2x_contrib','ind_price_2x_contrib']:\n",
        "    agg_df[i] = agg_df[i] + (agg_df['ind_tsr_2x']-agg_df['ind_mc_adj_2x_growth'])/4.0\n",
        "\n",
        "agg_df['ind_sales_3x_contrib'] = agg_df['ind_sales_3x_growth']\n",
        "agg_df['ind_earnings_3x_contrib'] = agg_df['ind_earnings_3x_growth'] - agg_df['ind_sales_3x_contrib']\n",
        "agg_df['ind_earnings_3x_contrib'] = np.where(agg_df['ind_earnings_3x_contrib']>0.5, 0.5, agg_df['ind_earnings_3x_contrib'])\n",
        "agg_df['ind_earnings_3x_contrib'] = np.where(agg_df['ind_earnings_3x_contrib']<-0.5, -0.5, agg_df['ind_earnings_3x_contrib'])  # cap earnings contribution at 50% - arbitrary but need to cut off the 6x growth contribution\n",
        "agg_df['ind_cash_3x_contrib'] = agg_df['ind_mc_adj_3x_growth'] - agg_df['ind_mc_3x_growth']\n",
        "agg_df['ind_price_3x_contrib'] = agg_df['ind_mc_adj_3x_growth'] - agg_df['ind_sales_3x_contrib'] - agg_df['ind_earnings_3x_contrib'] - agg_df['ind_cash_3x_contrib']\n",
        "\n",
        "for i in ['ind_sales_3x_contrib','ind_earnings_3x_contrib','ind_cash_3x_contrib','ind_price_3x_contrib']:\n",
        "    agg_df[i] = agg_df[i] + (agg_df['ind_tsr_3x']-agg_df['ind_mc_adj_3x_growth'])/4.0\n",
        "\n",
        "agg_df['ind_sales_4x_contrib'] = agg_df['ind_sales_4x_growth']\n",
        "agg_df['ind_earnings_4x_contrib'] = agg_df['ind_earnings_4x_growth'] - agg_df['ind_sales_4x_contrib']\n",
        "agg_df['ind_earnings_4x_contrib'] = np.where(agg_df['ind_earnings_4x_contrib']>0.5, 0.5, agg_df['ind_earnings_4x_contrib'])\n",
        "agg_df['ind_earnings_4x_contrib'] = np.where(agg_df['ind_earnings_4x_contrib']<-0.5, -0.5, agg_df['ind_earnings_4x_contrib'])  # cap earnings contribution at 50% - arbitrary but need to cut off the 6x growth contribution\n",
        "agg_df['ind_cash_4x_contrib'] = agg_df['ind_mc_adj_4x_growth'] - agg_df['ind_mc_4x_growth']\n",
        "agg_df['ind_price_4x_contrib'] = agg_df['ind_mc_adj_4x_growth'] - agg_df['ind_sales_4x_contrib'] - agg_df['ind_earnings_4x_contrib'] - agg_df['ind_cash_4x_contrib']\n",
        "\n",
        "for i in ['ind_sales_4x_contrib','ind_earnings_4x_contrib','ind_cash_4x_contrib','ind_price_4x_contrib']:\n",
        "    agg_df[i] = agg_df[i] + (agg_df['ind_tsr_4x']-agg_df['ind_mc_adj_4x_growth'])/4.0\n",
        "\n",
        "agg_df['ind_sales_5x_contrib'] = agg_df['ind_sales_5x_growth']\n",
        "agg_df['ind_earnings_5x_contrib'] = agg_df['ind_earnings_5x_growth'] - agg_df['ind_sales_5x_contrib']\n",
        "agg_df['ind_earnings_5x_contrib'] = np.where(agg_df['ind_earnings_5x_contrib']>0.5, 0.5, agg_df['ind_earnings_5x_contrib'])\n",
        "agg_df['ind_earnings_5x_contrib'] = np.where(agg_df['ind_earnings_5x_contrib']<-0.5, -0.5, agg_df['ind_earnings_5x_contrib'])\n",
        "        # cap earnings contribution at 50% - arbitrary but need to cut off the 6x growth contribution\n",
        "agg_df['ind_cash_5x_contrib'] = agg_df['ind_mc_adj_5x_growth'] - agg_df['ind_mc_5x_growth']\n",
        "agg_df['ind_price_5x_contrib'] = agg_df['ind_mc_adj_5x_growth'] - agg_df['ind_sales_5x_contrib'] - agg_df['ind_earnings_5x_contrib'] - agg_df['ind_cash_5x_contrib']\n",
        "\n",
        "for i in ['ind_sales_5x_contrib','ind_earnings_5x_contrib','ind_cash_5x_contrib','ind_price_5x_contrib']:\n",
        "    agg_df[i] = agg_df[i] + (agg_df['ind_tsr_5x']-agg_df['ind_mc_adj_5x_growth'])/4.0\n",
        "\n",
        "\n",
        "agg_df.tail(10)\n",
        "\n",
        "# prep for merging:\n",
        "ciq_df.rename(columns={'sp_adj_1x_growth':'tsr_1x','sp_adj_2x_growth':'tsr_2x','sp_adj_3x_growth':'tsr_3x','sp_adj_4x_growth':'tsr_4x','sp_adj_5x_growth':'tsr_5x'}, inplace=True)\n",
        "\n",
        "ciq_list = ['ciq_id','sector','start_year','tsr_1x','tsr_2x','tsr_3x','tsr_4x','tsr_5x']\n",
        "for i in ciq_df.columns[70:]:\n",
        "    ciq_list.append(i)\n",
        "ind_list = ['sector','start_year','ind_tsr_1x','ind_tsr_2x','ind_tsr_3x','ind_tsr_4x','ind_tsr_5x']\n",
        "for i in agg_df.columns[52:]:\n",
        "    ind_list.append(i)\n",
        "\n",
        "\n",
        "ciq_merge = ciq_df[ciq_list]\n",
        "ind_merge = agg_df[ind_list]\n",
        "\n",
        "merge_df = ciq_merge.merge(ind_merge, how='left', on=['sector','start_year'])\n",
        "merge_df.head(20)\n",
        "\n",
        "\"\"\"### MERGE INTO DOWNTURN DATABASE:\"\"\"\n",
        "\n",
        "df_decline = pickle.load(open('ceo_output_20231127_decline.p', 'rb'))\n",
        "\n",
        "#df_out = df_decline[['ciq_id','start_year','ceo_bool','name_x','cat_2','cat_1x','cat_5x']]\n",
        "df_out = df_decline[['ciq_id','start_year','ceo_bool','cat_2','cat_1x','cat_5x']]\n",
        "df_out = df_out.merge(merge_df, how='left', on=['ciq_id','start_year'])\n",
        "\n",
        "df_out.describe(include='all')\n",
        "\n",
        "df_out.to_csv('tsr_paths_20221127_v2.csv', index=False)"
      ],
      "metadata": {
        "id": "6_Jkw0nzpRTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. TURBULENCE ANALYSIS"
      ],
      "metadata": {
        "id": "P5XpIAKBln5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries for data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# Setting display options for pandas DataFrame to show 99 rows and 99 columns at maximum\n",
        "pd.options.display.max_rows = 99\n",
        "pd.options.display.max_columns = 99\n",
        "\n",
        "# Defining the industry column name for grouping and analysis; this is set as 'sector_2' to represent a particular level of industry aggregation\n",
        "industry = 'sector_2'\n",
        "\n",
        "# Reading the input data file 'turbulence_input_v3.csv' into a pandas DataFrame df\n",
        "df = pd.read_csv('turbulence_input_v3.csv')\n",
        "\n",
        "# Print statement to check the first few rows of the data\n",
        "print(df.head())\n",
        "\n",
        "# Convert all relevant columns to numeric, coercing errors (i.e., non-convertible values) to NaN\n",
        "cols_to_convert = ['sp', 'sp_adj', 'sales', 'ebitda', 'eps', 'capex', 'mc', 'r&d', 'restructuring']\n",
        "df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Print statement to check the data types of columns\n",
        "print(df.dtypes)\n",
        "\n",
        "# Creating 'sales_rank' column by ranking companies within each industry and year based on their sales; ranking is in descending order so higher sales get lower rank\n",
        "df['sales_rank'] = df.groupby(['start_year', industry])['sales'].rank(method='min', ascending=False)\n",
        "\n",
        "# Creating 'mc_rank' column by ranking companies within each industry and year based on their market capitalization; ranking is in descending order so higher market cap gets lower rank\n",
        "df['mc_rank'] = df.groupby(['start_year', industry])['mc'].rank(method='min', ascending=False)\n",
        "\n",
        "# Replacing ranks with NaN for companies with sales or market capitalization less or equal to zero as these values don't provide meaningful rank\n",
        "df['sales_rank'] = np.where(df['sales']<=0, np.NaN, df['sales_rank'])\n",
        "df['mc_rank'] = np.where(df['mc']<=0, np.NaN, df['mc_rank'])\n",
        "\n",
        "# Check for any NaN values in the sales and mc columns\n",
        "print(df[['sales', 'mc']].isnull().sum())\n",
        "\n",
        "# Check for any NaN values in the rank columns\n",
        "print(df[['sales_rank', 'mc_rank']].isnull().sum())\n",
        "\n",
        "# Print statement to check the values in the 'sales_rank' and 'mc_rank' columns\n",
        "print(df[['sales_rank', 'mc_rank']].head())\n",
        "\n",
        "# Creating lagged rank columns 'sales_rank_1' and 'mc_rank_1' to store previous year's rank for each company\n",
        "df['sales_rank_1'] = df.groupby(['ciq_id'])['sales_rank'].transform(lambda x: x.shift(1))\n",
        "df['mc_rank_1'] = df.groupby(['ciq_id'])['mc_rank'].transform(lambda x: x.shift(1))\n",
        "\n",
        "# Calculating the absolute difference in ranks between current and previous year for both sales and market capitalization\n",
        "df['sales_diff'] = np.where(df['sales_rank'] < df['sales_rank_1'], df['sales_rank_1']-df['sales_rank'], df['sales_rank']-df['sales_rank_1'])\n",
        "df['mc_diff'] = np.where(df['mc_rank'] < df['mc_rank_1'], df['mc_rank_1']-df['mc_rank'], df['mc_rank']-df['mc_rank_1'])\n",
        "\n",
        "# Getting the maximum rank between current and previous year for both sales and market capitalization to use in turbulence score calculation\n",
        "df['sales_max'] = np.where(df['sales_rank'] < df['sales_rank_1'], df['sales_rank_1'], df['sales_rank'])\n",
        "df['mc_max'] = np.where(df['mc_rank'] < df['mc_rank_1'], df['mc_rank_1'], df['mc_rank'])\n",
        "\n",
        "# Print statement to check for any zero or NaN values in 'sales_max' and 'mc_max' columns\n",
        "print((df['sales_max'] == 0).sum(), (df['sales_max'].isna().sum()))\n",
        "print((df['mc_max'] == 0).sum(), (df['mc_max'].isna().sum()))\n",
        "\n",
        "# Calculating turbulence scores for sales and market capitalization by dividing absolute rank difference by maximum rank\n",
        "df['sales_max'] = df['sales_max'].replace(0, np.nan)\n",
        "df['mc_max'] = df['mc_max'].replace(0, np.nan)\n",
        "\n",
        "df['sales_turb'] = df['sales_diff']/df['sales_max']\n",
        "df['mc_turb'] = df['mc_diff']/df['mc_max']\n",
        "\n",
        "# Print statement to check the values in the 'sales_turb' and 'mc_turb' columns\n",
        "print(df[['sales_turb', 'mc_turb']].head())\n",
        "\n",
        "# Saving intermediate results to 'turbulence_testing.csv'\n",
        "df.to_csv('turbulence_testing.csv')\n",
        "\n",
        "# Calculating mean turbulence score for sales and market capitalization per industry per year\n",
        "df_sales = df.groupby(['start_year', industry])['sales_turb'].mean()\n",
        "df_mc = df.groupby(['start_year', industry])['mc_turb'].mean()\n",
        "\n",
        "# Preparing the output DataFrame df_out by merging calculated mean turbulence scores back to the original data\n",
        "df_out = df[['INDEX', 'start_year', 'ciq_id', 'name', 'sector', 'sector_2', 'sector_3', 'sp', 'sp_adj', 'sales', 'mc']]\\\n",
        "                        .merge(df_sales.to_frame().reset_index(), how='left', on=['start_year', industry])\\\n",
        "                        .merge(df_mc.to_frame().reset_index(), how='left', on=['start_year', industry])\n",
        "\n",
        "# Calculating mean and standard deviation of turbulence scores for normalization\n",
        "sales_mean = df_out['sales_turb'].mean()\n",
        "mc_mean = df_out['mc_turb'].mean()\n",
        "sales_std = df_out['sales_turb'].std()\n",
        "mc_std = df_out['mc_turb'].std()\n",
        "\n",
        "# Calculating a combined turbulence score by normalizing and summing the sales and market cap turbulence scores\n",
        "df_out['combo_turb'] = (df_out['sales_turb'] - sales_mean)/sales_std + (df_out['mc_turb']-mc_mean)/mc_std\n",
        "\n",
        "# Creating a rolling window of size 5 to calculate 5-year average turbulence scores\n",
        "df_roll = df_out.groupby('ciq_id')[['sales_turb', 'mc_turb', 'combo_turb']].rolling(5).mean()\n",
        "df_roll.index = df_roll.index.droplevel()  # Dropping 'ciq_id' from index to match the structure of df_out\n",
        "\n",
        "# Merging the 5-year average turbulence scores back to df_out\n",
        "df_out['sales_turb_avg5'] = df_roll['sales_turb']\n",
        "df_out['mc_turb_avg5'] = df_roll['mc_turb']\n",
        "\n",
        "# Calculating mean and standard deviation of 5-year average turbulence scores for normalization\n",
        "sales_mean_5 = df_out['sales_turb_avg5'].mean()\n",
        "mc_mean_5 = df_out['mc_turb_avg5'].mean()\n",
        "sales_std_5 = df_out['sales_turb_avg5'].std()\n",
        "mc_std_5 = df_out['mc_turb_avg5'].std()\n",
        "\n",
        "# Calculating a combined 5-year average turbulence score by normalizing and summing the 5-year average sales and market cap turbulence scores\n",
        "df_out['combo_turb_avg5'] = (df_out['sales_turb_avg5'] - sales_mean_5)/sales_std_5 + (df_out['mc_turb_avg5']-mc_mean_5)/mc_std_5\n",
        "\n",
        "# Viewing the first 25 rows of df_out to check the data\n",
        "df_out.head(25)\n",
        "\n",
        "# Describing df_out to get a summary of statistics\n",
        "df_out.describe()\n",
        "\n",
        "# Saving the final output to 'turbulence_analysis_v3.csv'\n",
        "df_out.to_csv('turbulence_analysis_v3.csv', index=False)"
      ],
      "metadata": {
        "id": "wZEEEALKpVHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. TRANSFORMATION REGRESSIONS"
      ],
      "metadata": {
        "id": "b-ua0h22loHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Files needed: ceo_output_decline, ceo_tf_corrections, investment_analysis, tf_restructuring_input, company_id,\n",
        "\n",
        "# Importing necessary libraries for data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "# Setting display options to allow for viewing more rows and columns in the DataFrame\n",
        "pd.options.display.max_rows = 99\n",
        "pd.options.display.max_columns = 99\n",
        "\n",
        "# Loading the DataFrame of 'decline' firms from a pickled file\n",
        "df_decline = pickle.load(open(\"ceo_output_20231127_decline.p\", \"rb\" ))\n",
        "\n",
        "# Selecting relevant columns from the DataFrame\n",
        "df_decline = df_decline[['ciq_id','start_year','name_x','sector','sp','sp_adj','sales','mc','tsr_5p','tsr_2','tsr_1x','tsr_5x','tsr_5p_delta','tsr_2_delta','tsr_1x_delta','tsr_5x_delta','tsr_2_chg','tsr_1x_chg','tsr_5x_chg','cat_2','cat_1x','cat_5x','net_longterm','net_biological','net_longterm_avg','net_bio_avg']].rename(columns={'name_x':'name'})\n",
        "\n",
        "# Adding a column to indicate if a firm should be excluded based on a condition\n",
        "df_decline['exclude'] = np.where((df_decline['tsr_5x']==-1), \"Y\",\"N\")\n",
        "\n",
        "# Generating descriptive statistics of the DataFrame\n",
        "df_decline.describe(include='all')\n",
        "\n",
        "# Loading corrected CEO data from a CSV file\n",
        "ceo_change = pd.read_csv('ceo_tf_corrections.csv', index_col=False)\n",
        "# Adding a boolean column to indicate if there's a CEO change\n",
        "ceo_change['ceo_bool'] = np.where((pd.isnull(ceo_change['ceo']) & pd.isnull(ceo_change['ceo_1x'])), 0,1)\n",
        "# Adding a boolean column to indicate if the CEO is an outsider\n",
        "ceo_change['outsider_bool'] = np.where(((pd.isnull(ceo_change['prev_title']) & pd.isnull(ceo_change['ceo_title_1x'])) & (ceo_change['ceo_bool']==1)), 1,0)\n",
        "\n",
        "# Viewing the first 20 rows of the CEO data\n",
        "ceo_change.head(20)\n",
        "\n",
        "# Loading investment data from a CSV file\n",
        "df_inv = pd.read_csv('investment_analysis_20231122.csv', index_col=False)\n",
        "# Selecting relevant columns from the investment data\n",
        "df_inv = df_inv[['ciq_id','start_year','name_x','capex_sales_delta','capex_sales_delta_avg','capex_sales_delta_std','rd_sales_delta','rd_sales_delta_avg','rd_sales_delta_std']]\n",
        "\n",
        "# Generating descriptive statistics of the investment data\n",
        "df_inv.describe(include='all')\n",
        "\n",
        "# Loading restructuring data from a CSV file\n",
        "df_restrux = pd.read_csv('tf_restructuring_input_20170810.csv', index_col=False)\n",
        "\n",
        "# Correcting the sign of restructuring values\n",
        "df_restrux['restructuring'] = df_restrux['restructuring']*-1\n",
        "df_restrux['restructuring_1x'] = df_restrux['restructuring_1x']*-1\n",
        "\n",
        "# Calculating the restructuring to sales ratio\n",
        "df_restrux['restrux_sales'] = df_restrux['restructuring']/df_restrux['sales']\n",
        "df_restrux['restrux_sales_1x'] = df_restrux['restructuring_1x']/df_restrux['sales_1x']\n",
        "\n",
        "# Adding a boolean column to indicate if a firm has undergone a transformation\n",
        "df_restrux['tf_bool'] = df_restrux[['tf', 'tf_1x']].max(axis=1)\n",
        "# Calculating the size of restructuring based on the maximum of the two periods\n",
        "df_restrux['restrux_size'] = df_restrux[['restrux_sales', 'restrux_sales_1x']].max(axis=1)\n",
        "\n",
        "# Calculating the mean and standard deviation of restructuring size for transformed firms\n",
        "restrux_mean = df_restrux[df_restrux['tf_bool']==1]['restrux_size'].mean()\n",
        "restrux_std = df_restrux[df_restrux['tf_bool']==1]['restrux_size'].std()\n",
        "# Standardizing the restructuring size\n",
        "df_restrux['restrux_size_std'] = df_restrux['restrux_size'].apply(lambda x: (x - restrux_mean)/restrux_std)\n",
        "\n",
        "# Generating descriptive statistics of the restructuring data\n",
        "df_restrux.describe(include='all')\n",
        "\n",
        "# Loading other structural data from a CSV file\n",
        "df_helper = pd.read_csv('231120_company_id.csv')[['ciq_id','yr_founded']]\n",
        "# Converting the year founded to numeric\n",
        "df_helper['yr_founded'] = pd.to_numeric(df_helper['yr_founded'])\n",
        "\n",
        "# Generating descriptive statistics of the structural data\n",
        "df_helper.describe(include='all')\n",
        "\n",
        "# Merging all data together for final analysis\n",
        "df_final = df_decline[df_decline['exclude']=='N'] \\\n",
        "    .merge(ceo_change[['start_year','ciq_id','ceo_bool','outsider_bool']], how='left', on=['start_year','ciq_id']) \\\n",
        "    .merge(df_inv.drop('name_x', axis=1), how='left', on=['start_year','ciq_id']) \\\n",
        "    .merge(df_restrux[['start_year','ciq_id','tf_bool','restrux_size','restrux_size_std']], how='left', on=['start_year','ciq_id']) \\\n",
        "    .merge(df_helper, how='left', on='ciq_id')\n",
        "\n",
        "# Calculating the log of firm age and sales for normalization\n",
        "df_final['l_age'] = np.log((df_final['start_year'] - df_final['yr_founded'] + 1))\n",
        "df_final['l_sales'] = np.log((df_final['sales']))\n",
        "\n",
        "# Calculating the mean and standard deviation for normalization\n",
        "age_mean = df_final['l_age'].mean()\n",
        "age_std = df_final['l_age'].std()\n",
        "df_final['l_age_std'] = df_final['l_age'].apply(lambda x: (x - age_mean)/age_std)\n",
        "sales_mean = df_final['l_sales'].mean()\n",
        "sales_std = df_final['l_sales'].std()\n",
        "df_final['l_sales_std'] = df_final['l_sales'].apply(lambda x: (x - sales_mean)/sales_std)\n",
        "\n",
        "# Generating descriptive statistics of the final DataFrame\n",
        "df_final.describe(include='all')\n",
        "\n",
        "# Converting certain numeric variables to binary for further analysis\n",
        "std_vars = ['capex_sales_delta_avg', 'rd_sales_delta_avg', 'restrux_size_std','l_age_std','l_sales_std','net_longterm']\n",
        "vars_new = ['capex_bool','rd_bool','restrux_bool','age_bool','sales_bool','lt_bool']\n",
        "for i in range(6):\n",
        "    df_final[str(vars_new[i])] = df_final[str(std_vars[i])].apply(lambda x: x>0)\n",
        "\n",
        "\n",
        "\n",
        "# Generating descriptive statistics of the final DataFrame after binary conversion\n",
        "df_final.describe(include='all')\n",
        "\n",
        "# Saving the final DataFrame to a CSV file for further analysis\n",
        "df_final.to_csv('tf_regression_data_20170810.csv', index=False)"
      ],
      "metadata": {
        "id": "j--BXzxrpV1Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}